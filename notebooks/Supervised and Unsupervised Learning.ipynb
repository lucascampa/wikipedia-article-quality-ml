{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as _hex_pandas\n",
        "import datetime as _hex_datetime\n",
        "import json as _hex_json"
      ],
      "execution_count": null,
      "metadata": {
        "id": "_cD-VOLElRci"
      },
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hex_scheduled = _hex_json.loads(\"false\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "otrYc-ialRcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hex_user_email = _hex_json.loads(\"\\\"example-user@example.com\\\"\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "9I4yyJJPlRco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hex_user_attributes = _hex_json.loads(\"{}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "MnEzAkCTlRco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hex_run_context = _hex_json.loads(\"\\\"logic\\\"\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "trPaiLHklRcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hex_timezone = _hex_json.loads(\"\\\"UTC\\\"\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "cmjC0kxXlRcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hex_project_id = _hex_json.loads(\"\\\"019be885-c92a-7aac-b4fd-d91fd28ebe86\\\"\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "uvagH0IflRcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hex_project_name = _hex_json.loads(\"\\\"Supervised and Unsupervised Learning\\\"\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "8Uuk__EmlRcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hex_status = _hex_json.loads(\"\\\"\\\"\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "NdWT4vvrlRcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hex_categories = _hex_json.loads(\"[]\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "U64PAIQulRcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hex_color_palette = _hex_json.loads(\"[\\\"#4C78A8\\\",\\\"#F58518\\\",\\\"#E45756\\\",\\\"#72B7B2\\\",\\\"#54A24B\\\",\\\"#EECA3B\\\",\\\"#B279A2\\\",\\\"#FF9DA6\\\",\\\"#9D755D\\\",\\\"#BAB0AC\\\"]\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "oiEelSJilRcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('wikipedia.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "aET26JlMlRcu",
        "outputId": "7be8e5bd-6cc7-405f-aac7-21138e8499c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.hex.export+parquet": {
              "success": true,
              "exportKey": "019bae66-d350-7001-a251-d171be1cf243/019be885-c92a-7aac-b4fd-d91fd28ebe86/exports/019bec6f-564a-788b-b35d-becc803e2e9f"
            },
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>page_id</th>\n      <th>title</th>\n      <th>url</th>\n      <th>content_length</th>\n      <th>num_links</th>\n      <th>num_sections</th>\n      <th>num_references</th>\n      <th>all_categories</th>\n      <th>meta_categories</th>\n      <th>science_categories</th>\n      <th>num_categories</th>\n      <th>summary_length</th>\n      <th>avg_sentence_length</th>\n      <th>last_revision_date</th>\n      <th>num_languages</th>\n      <th>languages</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>47262</td>\n      <td>2 Pallas</td>\n      <td>https://en.wikipedia.org/wiki/2_Pallas</td>\n      <td>49731</td>\n      <td>184</td>\n      <td>16</td>\n      <td>138</td>\n      <td>['1802 in science', 'Articles containing Ancie...</td>\n      <td>['Articles containing Ancient Greek (to 1453)-...</td>\n      <td>['1802 in science', 'Astronomical objects disc...</td>\n      <td>20</td>\n      <td>2198</td>\n      <td>24.40</td>\n      <td>2025-11-25T20:44:17Z</td>\n      <td>92</td>\n      <td>['af', 'gsw', 'an', 'anp', 'ar', 'arz', 'ast',...</td>\n      <td>GA</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>71318</td>\n      <td>100-year flood</td>\n      <td>https://en.wikipedia.org/wiki/100-year_flood</td>\n      <td>24708</td>\n      <td>265</td>\n      <td>10</td>\n      <td>43</td>\n      <td>['Actuarial science', 'Articles with short des...</td>\n      <td>['Articles with short description', 'CS1 maint...</td>\n      <td>['Actuarial science', 'Extreme value data', 'F...</td>\n      <td>9</td>\n      <td>698</td>\n      <td>25.20</td>\n      <td>2025-12-01T13:21:16Z</td>\n      <td>9</td>\n      <td>['ar', 'de', 'es', 'fa', 'fr', 'ms', 'nb', 'sv...</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>175149</td>\n      <td>'Pataphysics</td>\n      <td>https://en.wikipedia.org/wiki/%27Pataphysics</td>\n      <td>49610</td>\n      <td>51</td>\n      <td>24</td>\n      <td>65</td>\n      <td>[\"'Pataphysics\", 'Alfred Jarry', 'All articles...</td>\n      <td>['All articles containing potentially dated st...</td>\n      <td>[\"'Pataphysics\", 'Alfred Jarry', 'Fictional ph...</td>\n      <td>19</td>\n      <td>273</td>\n      <td>20.50</td>\n      <td>2026-01-04T15:47:15Z</td>\n      <td>24</td>\n      <td>['ast', 'bg', 'ca', 'cs', 'de', 'el', 'es', 'e...</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>786020</td>\n      <td>1,4-Butanediol</td>\n      <td>https://en.wikipedia.org/wiki/1,4-Butanediol</td>\n      <td>19934</td>\n      <td>0</td>\n      <td>12</td>\n      <td>32</td>\n      <td>['1,4-Butanediyl compounds', 'Alcohol solvents...</td>\n      <td>['All articles needing additional references',...</td>\n      <td>['1,4-Butanediyl compounds', 'Alcohol solvents...</td>\n      <td>25</td>\n      <td>369</td>\n      <td>25.00</td>\n      <td>2026-01-08T01:18:38Z</td>\n      <td>26</td>\n      <td>['ar', 'az', 'azb', 'ca', 'de', 'eo', 'es', 'e...</td>\n      <td>B</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1110853</td>\n      <td>*Dyēus</td>\n      <td>https://en.wikipedia.org/wiki/*Dy%C4%93us</td>\n      <td>62518</td>\n      <td>0</td>\n      <td>18</td>\n      <td>60</td>\n      <td>['Articles containing Albanian-language text',...</td>\n      <td>['Articles containing Albanian-language text',...</td>\n      <td>['Etymologies', 'Proto-Indo-European gods', 'R...</td>\n      <td>28</td>\n      <td>841</td>\n      <td>20.67</td>\n      <td>2025-12-27T19:49:48Z</td>\n      <td>18</td>\n      <td>['ar', 'ca', 'cs', 'es', 'ext', 'fa', 'fi', 'f...</td>\n      <td>B</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>16467</th>\n      <td>5065997</td>\n      <td>Zirconium(IV) silicate</td>\n      <td>https://en.wikipedia.org/wiki/Zirconium(IV)_si...</td>\n      <td>6474</td>\n      <td>102</td>\n      <td>5</td>\n      <td>9</td>\n      <td>['Articles containing unverified chemical info...</td>\n      <td>['Articles containing unverified chemical info...</td>\n      <td>['High-κ dielectrics', 'Refractory materials',...</td>\n      <td>16</td>\n      <td>391</td>\n      <td>8.57</td>\n      <td>2025-09-24T00:32:38Z</td>\n      <td>14</td>\n      <td>['ar', 'azb', 'cs', 'de', 'es', 'fa', 'fr', 'i...</td>\n      <td>START</td>\n    </tr>\n    <tr>\n      <th>16468</th>\n      <td>24215517</td>\n      <td>Zirconium(IV) sulfate</td>\n      <td>https://en.wikipedia.org/wiki/Zirconium(IV)_su...</td>\n      <td>4083</td>\n      <td>346</td>\n      <td>3</td>\n      <td>4</td>\n      <td>['Articles containing unverified chemical info...</td>\n      <td>['Articles containing unverified chemical info...</td>\n      <td>['Sulfates', 'Zirconium(IV) compounds']</td>\n      <td>14</td>\n      <td>354</td>\n      <td>15.00</td>\n      <td>2025-06-24T01:25:10Z</td>\n      <td>14</td>\n      <td>['ar', 'azb', 'bn', 'de', 'fa', 'fi', 'fr', 'i...</td>\n      <td>STUB</td>\n    </tr>\n    <tr>\n      <th>16469</th>\n      <td>34411</td>\n      <td>Zodiac</td>\n      <td>https://en.wikipedia.org/wiki/Zodiac</td>\n      <td>67343</td>\n      <td>500</td>\n      <td>18</td>\n      <td>99</td>\n      <td>['All Wikipedia articles written in American E...</td>\n      <td>['All Wikipedia articles written in American E...</td>\n      <td>['Ancient astronomy', 'Astrological signs', 'A...</td>\n      <td>32</td>\n      <td>1703</td>\n      <td>24.91</td>\n      <td>2025-12-17T05:15:17Z</td>\n      <td>107</td>\n      <td>['af', 'ar', 'arz', 'ast', 'az', 'ba', 'bcl', ...</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>16470</th>\n      <td>34527</td>\n      <td>Zodiacal light</td>\n      <td>https://en.wikipedia.org/wiki/Zodiacal_light</td>\n      <td>22395</td>\n      <td>360</td>\n      <td>10</td>\n      <td>35</td>\n      <td>['All Wikipedia articles in need of updating',...</td>\n      <td>['All Wikipedia articles in need of updating',...</td>\n      <td>['Light sources', 'Observational astronomy', '...</td>\n      <td>11</td>\n      <td>1697</td>\n      <td>24.82</td>\n      <td>2026-01-23T05:19:23Z</td>\n      <td>55</td>\n      <td>['af', 'ar', 'ary', 'az', 'bg', 'ca', 'ckb', '...</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>16471</th>\n      <td>351466</td>\n      <td>Zonda wind</td>\n      <td>https://en.wikipedia.org/wiki/Zonda_wind</td>\n      <td>2868</td>\n      <td>21</td>\n      <td>5</td>\n      <td>0</td>\n      <td>['All articles needing coordinates', 'Argentin...</td>\n      <td>['All articles needing coordinates', 'Argentin...</td>\n      <td>['Climate of Argentina', 'Föhn effect', 'Mendo...</td>\n      <td>11</td>\n      <td>139</td>\n      <td>25.00</td>\n      <td>2025-07-27T06:43:28Z</td>\n      <td>19</td>\n      <td>['ar', 'be', 'bh', 'ca', 'de', 'es', 'fr', 'ha...</td>\n      <td>START</td>\n    </tr>\n  </tbody>\n</table>\n<p>16472 rows × 17 columns</p>\n</div>"
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "from collections import Counter\n",
        "\n",
        "# Parse all lists to count unique values\n",
        "all_langs = []\n",
        "all_cats = []\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    langs = ast.literal_eval(row['languages'])\n",
        "    all_langs.extend(langs)\n",
        "\n",
        "    cats = ast.literal_eval(row['all_categories'])\n",
        "    all_cats.extend(cats)\n",
        "\n",
        "# Count uniques\n",
        "unique_langs = set(all_langs)\n",
        "unique_cats = set(all_cats)\n",
        "lang_freq = Counter(all_langs)\n",
        "cat_freq = Counter(all_cats)\n",
        "\n",
        "print(f\"DATASET OVERVIEW\")\n",
        "print(f\"=\" * 80)\n",
        "print(f\"Total articles: {len(df):,}\")\n",
        "print(f\"Unique languages: {len(unique_langs):,}\")\n",
        "print(f\"Unique categories: {len(unique_cats):,}\")\n",
        "print(f\"\\nPotential one-hot encoded features:\")\n",
        "print(f\"  Languages: {len(unique_langs):,} binary columns\")\n",
        "print(f\"  Categories: {len(unique_cats):,} binary columns\")\n",
        "print(f\"  TOTAL: {len(unique_langs) + len(unique_cats):,} new features\")\n",
        "\n",
        "print(f\"\\n\" + \"=\" * 80)\n",
        "print(f\"LANGUAGE DISTRIBUTION\")\n",
        "print(f\"=\" * 80)\n",
        "print(f\"Top 20 most common languages:\")\n",
        "for lang, count in lang_freq.most_common(20):\n",
        "    pct = count / len(df) * 100\n",
        "    print(f\"  {lang:5} → {count:3} articles ({pct:5.1f}%)\")\n",
        "\n",
        "print(f\"\\n\" + \"=\" * 80)\n",
        "print(f\"CATEGORY DISTRIBUTION\")\n",
        "print(f\"=\" * 80)\n",
        "print(f\"Top 30 most common categories:\")\n",
        "for cat, count in cat_freq.most_common(30):\n",
        "    pct = count / len(df) * 100\n",
        "    print(f\"  {cat[:60]:60} → {count:3} articles ({pct:5.1f}%)\")"
      ],
      "metadata": {
        "id": "LneCorQslRcw",
        "outputId": "37475e5e-fd67-471a-a6fa-c47c769ffecd"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": "DATASET OVERVIEW\n================================================================================\nTotal articles: 16,472\nUnique languages: 342\nUnique categories: 22,878\n\nPotential one-hot encoded features:\n  Languages: 342 binary columns\n  Categories: 22,878 binary columns\n  TOTAL: 23,220 new features\n\n================================================================================\nLANGUAGE DISTRIBUTION\n================================================================================\nTop 20 most common languages:\n  es    → 4852 articles ( 29.5%)\n  ar    → 4830 articles ( 29.3%)\n  fr    → 4828 articles ( 29.3%)\n  de    → 4778 articles ( 29.0%)\n  fa    → 4761 articles ( 28.9%)\n  zh    → 4571 articles ( 27.8%)\n  ru    → 4473 articles ( 27.2%)\n  ja    → 4464 articles ( 27.1%)\n  ca    → 4378 articles ( 26.6%)\n  pt    → 4353 articles ( 26.4%)\n  ko    → 4349 articles ( 26.4%)\n  it    → 4322 articles ( 26.2%)\n  uk    → 4210 articles ( 25.6%)\n  pl    → 4063 articles ( 24.7%)\n  nl    → 4049 articles ( 24.6%)\n  cs    → 3935 articles ( 23.9%)\n  id    → 3845 articles ( 23.3%)\n  fi    → 3752 articles ( 22.8%)\n  sv    → 3619 articles ( 22.0%)\n  tr    → 3554 articles ( 21.6%)\n\n================================================================================\nCATEGORY DISTRIBUTION\n================================================================================\nTop 30 most common categories:\n  Articles with short description                              → 15912 articles ( 96.6%)\n  Short description is different from Wikidata                 → 10260 articles ( 62.3%)\n  Short description matches Wikidata                           → 5948 articles ( 36.1%)\n  All articles with unsourced statements                       → 5751 articles ( 34.9%)\n  Webarchive template wayback links                            → 4990 articles ( 30.3%)\n  All articles needing additional references                   → 3180 articles ( 19.3%)\n  Commons category link is on Wikidata                         → 2915 articles ( 17.7%)\n  Articles containing unverified chemical infoboxes            → 1516 articles (  9.2%)\n  Commons category link from Wikidata                          → 1461 articles (  8.9%)\n  All articles with dead external links                        → 1299 articles (  7.9%)\n  ECHA InfoCard ID from Wikidata                               → 1279 articles (  7.8%)\n  CS1: long volume value                                       → 1227 articles (  7.4%)\n  CS1 maint: multiple names: authors list                      → 1189 articles (  7.2%)\n  Articles without KEGG source                                 → 1119 articles (  6.8%)\n  CS1 German-language sources (de)                             → 1118 articles (  6.8%)\n  CS1 errors: ISBN date                                        → 1077 articles (  6.5%)\n  All stub articles                                            → 1037 articles (  6.3%)\n  All Wikipedia articles written in American English           → 953 articles (  5.8%)\n  Articles with excerpts                                       → 888 articles (  5.4%)\n  Articles with permanently dead external links                → 884 articles (  5.4%)\n  Articles containing Latin-language text                      → 855 articles (  5.2%)\n  All articles containing potentially dated statements         → 835 articles (  5.1%)\n  CS1 maint: work parameter with ISBN                          → 786 articles (  4.8%)\n  CS1: unfit URL                                               → 778 articles (  4.7%)\n  CS1 French-language sources (fr)                             → 765 articles (  4.6%)\n  Chembox having GHS data                                      → 760 articles (  4.6%)\n  Articles without EBI source                                  → 729 articles (  4.4%)\n  CS1 maint: location missing publisher                        → 685 articles (  4.2%)\n  Chembox image size set                                       → 652 articles (  4.0%)\n  All articles lacking reliable references                     → 632 articles (  3.8%)\n"
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "from collections import Counter\n",
        "\n",
        "# Parse the three category types\n",
        "all_cats = []\n",
        "meta_cats = []\n",
        "science_cats = []\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    all_c = ast.literal_eval(row['all_categories'])\n",
        "    all_cats.extend(all_c)\n",
        "\n",
        "    meta_c = ast.literal_eval(row['meta_categories'])\n",
        "    meta_cats.extend(meta_c)\n",
        "\n",
        "    science_c = ast.literal_eval(row['science_categories'])\n",
        "    science_cats.extend(science_c)\n",
        "\n",
        "# Count frequencies\n",
        "all_freq = Counter(all_cats)\n",
        "meta_freq = Counter(meta_cats)\n",
        "science_freq = Counter(science_cats)\n",
        "\n",
        "# Get unique counts\n",
        "unique_all = set(all_cats)\n",
        "unique_meta = set(meta_cats)\n",
        "unique_science = set(science_cats)\n",
        "\n",
        "print(f\"CATEGORY TYPE COMPARISON\")\n",
        "print(f\"=\" * 80)\n",
        "print(f\"Total articles: {len(df):,}\\n\")\n",
        "\n",
        "print(f\"{'Category Type':<20} {'Unique':<10} {'Total Occurrences':<20} {'Avg per Article':<15}\")\n",
        "print(f\"{'-'*20} {'-'*10} {'-'*20} {'-'*15}\")\n",
        "print(f\"{'All Categories':<20} {len(unique_all):<10,} {len(all_cats):<20,} {len(all_cats)/len(df):<15.2f}\")\n",
        "print(f\"{'Meta Categories':<20} {len(unique_meta):<10,} {len(meta_cats):<20,} {len(meta_cats)/len(df):<15.2f}\")\n",
        "print(f\"{'Science Categories':<20} {len(unique_science):<10,} {len(science_cats):<20,} {len(science_cats)/len(df):<15.2f}\")\n",
        "\n",
        "print(f\"\\n{'Meta % of All:':<20} {len(unique_meta)/len(unique_all)*100:.1f}% unique, {len(meta_cats)/len(all_cats)*100:.1f}% of occurrences\")\n",
        "print(f\"{'Science % of All:':<20} {len(unique_science)/len(unique_all)*100:.1f}% unique, {len(science_cats)/len(all_cats)*100:.1f}% of occurrences\")\n",
        "\n",
        "print(f\"\\n\" + \"=\" * 80)\n",
        "print(f\"TOP 15 CATEGORIES BY TYPE\")\n",
        "print(f\"=\" * 80)\n",
        "\n",
        "print(f\"\\nALL CATEGORIES:\")\n",
        "for cat, count in all_freq.most_common(15):\n",
        "    pct = count / len(df) * 100\n",
        "    print(f\"  {cat[:60]:60} → {count:5,} ({pct:5.1f}%)\")\n",
        "\n",
        "print(f\"\\nMETA CATEGORIES:\")\n",
        "for cat, count in meta_freq.most_common(15):\n",
        "    pct = count / len(df) * 100\n",
        "    print(f\"  {cat[:60]:60} → {count:5,} ({pct:5.1f}%)\")\n",
        "\n",
        "print(f\"\\nSCIENCE CATEGORIES:\")\n",
        "for cat, count in science_freq.most_common(15):\n",
        "    pct = count / len(df) * 100\n",
        "    print(f\"  {cat[:60]:60} → {count:5,} ({pct:5.1f}%)\")"
      ],
      "metadata": {
        "id": "OoAsnrPLlRcx",
        "outputId": "999f3d3e-ccf8-476a-bdfe-eca4a8f1bbed"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": "CATEGORY TYPE COMPARISON\n================================================================================\nTotal articles: 16,472\n\nCategory Type        Unique     Total Occurrences    Avg per Article\n-------------------- ---------- -------------------- ---------------\nAll Categories       22,878     195,299              11.86          \nMeta Categories      6,052      135,319              8.22           \nScience Categories   16,826     59,980               3.64           \n\nMeta % of All:       26.5% unique, 69.3% of occurrences\nScience % of All:    73.5% unique, 30.7% of occurrences\n\n================================================================================\nTOP 15 CATEGORIES BY TYPE\n================================================================================\n\nALL CATEGORIES:\n  Articles with short description                              → 15,912 ( 96.6%)\n  Short description is different from Wikidata                 → 10,260 ( 62.3%)\n  Short description matches Wikidata                           → 5,948 ( 36.1%)\n  All articles with unsourced statements                       → 5,751 ( 34.9%)\n  Webarchive template wayback links                            → 4,990 ( 30.3%)\n  All articles needing additional references                   → 3,180 ( 19.3%)\n  Commons category link is on Wikidata                         → 2,915 ( 17.7%)\n  Articles containing unverified chemical infoboxes            → 1,516 (  9.2%)\n  Commons category link from Wikidata                          → 1,461 (  8.9%)\n  All articles with dead external links                        → 1,299 (  7.9%)\n  ECHA InfoCard ID from Wikidata                               → 1,279 (  7.8%)\n  CS1: long volume value                                       → 1,227 (  7.4%)\n  CS1 maint: multiple names: authors list                      → 1,189 (  7.2%)\n  Articles without KEGG source                                 → 1,119 (  6.8%)\n  CS1 German-language sources (de)                             → 1,118 (  6.8%)\n\nMETA CATEGORIES:\n  Articles with short description                              → 15,912 ( 96.6%)\n  Short description is different from Wikidata                 → 10,260 ( 62.3%)\n  Short description matches Wikidata                           → 5,948 ( 36.1%)\n  All articles with unsourced statements                       → 5,751 ( 34.9%)\n  Webarchive template wayback links                            → 4,990 ( 30.3%)\n  All articles needing additional references                   → 3,180 ( 19.3%)\n  Commons category link is on Wikidata                         → 2,915 ( 17.7%)\n  Articles containing unverified chemical infoboxes            → 1,516 (  9.2%)\n  Commons category link from Wikidata                          → 1,461 (  8.9%)\n  All articles with dead external links                        → 1,299 (  7.9%)\n  ECHA InfoCard ID from Wikidata                               → 1,279 (  7.8%)\n  CS1: long volume value                                       → 1,227 (  7.4%)\n  CS1 maint: multiple names: authors list                      → 1,189 (  7.2%)\n  Articles without KEGG source                                 → 1,119 (  6.8%)\n  CS1 German-language sources (de)                             → 1,118 (  6.8%)\n\nSCIENCE CATEGORIES:\n  Wikipedia articles incorporating text from the 20th edition  →   157 (  1.0%)\n  Pseudoscience                                                →   151 (  0.9%)\n  Metal halides                                                →   150 (  0.9%)\n  Architectural elements                                       →   129 (  0.8%)\n  E-number additives                                           →   114 (  0.7%)\n  Inorganic compound stubs                                     →   105 (  0.6%)\n  American inventions                                          →    94 (  0.6%)\n  Oxidizing agents                                             →    90 (  0.5%)\n  Chlorides                                                    →    88 (  0.5%)\n  Fluid dynamics                                               →    86 (  0.5%)\n  Fluorides                                                    →    82 (  0.5%)\n  Structural engineering                                       →    80 (  0.5%)\n  Physical quantities                                          →    79 (  0.5%)\n  Engineering disciplines                                      →    72 (  0.4%)\n  Hydrology                                                    →    71 (  0.4%)\n"
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "from collections import Counter\n",
        "\n",
        "# Calculate coverage for top-N items\n",
        "def calculate_coverage(df, column_name, freq_counter, top_n_list):\n",
        "    \"\"\"Calculate % of articles covered by top-N items\"\"\"\n",
        "    results = []\n",
        "\n",
        "    for n in top_n_list:\n",
        "        top_n_items = set([item for item, count in freq_counter.most_common(n)])\n",
        "\n",
        "        articles_covered = 0\n",
        "        articles_with_zero_top_n = 0\n",
        "\n",
        "        for idx, row in df.iterrows():\n",
        "            items = ast.literal_eval(row[column_name])\n",
        "\n",
        "            if any(item in top_n_items for item in items):\n",
        "                articles_covered += 1\n",
        "            elif len(items) > 0:\n",
        "                articles_with_zero_top_n += 1\n",
        "\n",
        "        coverage_pct = articles_covered / len(df) * 100\n",
        "        results.append((n, articles_covered, coverage_pct, articles_with_zero_top_n))\n",
        "\n",
        "    return results\n",
        "\n",
        "# Calculate frequencies for meta and science categories\n",
        "meta_cats = []\n",
        "science_cats = []\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    meta_c = ast.literal_eval(row['meta_categories'])\n",
        "    meta_cats.extend(meta_c)\n",
        "\n",
        "    science_c = ast.literal_eval(row['science_categories'])\n",
        "    science_cats.extend(science_c)\n",
        "\n",
        "meta_freq = Counter(meta_cats)\n",
        "science_freq = Counter(science_cats)\n",
        "\n",
        "# Check for articles with 0 languages\n",
        "articles_with_zero_langs = 0\n",
        "for idx, row in df.iterrows():\n",
        "    langs = ast.literal_eval(row['languages'])\n",
        "    if len(langs) == 0:\n",
        "        articles_with_zero_langs += 1\n",
        "\n",
        "print(f\"DATA QUALITY CHECK\")\n",
        "print(f\"=\" * 80)\n",
        "print(f\"Total articles: {len(df):,}\")\n",
        "print(f\"Articles with 0 languages: {articles_with_zero_langs:,} ({articles_with_zero_langs/len(df)*100:.1f}%)\")\n",
        "\n",
        "# Calculate coverage for languages\n",
        "print(f\"\\n\" + \"=\" * 80)\n",
        "print(f\"LANGUAGE COVERAGE\")\n",
        "print(f\"=\" * 80)\n",
        "lang_coverage = calculate_coverage(df, 'languages', lang_freq, [20, 30, 50, 100])\n",
        "for n, count, pct, zero_top_n in lang_coverage:\n",
        "    print(f\"Top {n:3} languages  → {count:5,} articles covered ({pct:5.1f}%) | {zero_top_n:,} with 0 top-{n} languages\")\n",
        "\n",
        "# Calculate coverage for meta categories\n",
        "print(f\"\\n\" + \"=\" * 80)\n",
        "print(f\"META CATEGORY COVERAGE\")\n",
        "print(f\"=\" * 80)\n",
        "meta_coverage = calculate_coverage(df, 'meta_categories', meta_freq, [20, 30, 50, 100])\n",
        "for n, count, pct, zero_top_n in meta_coverage:\n",
        "    print(f\"Top {n:3} meta cats   → {count:5,} articles covered ({pct:5.1f}%) | {zero_top_n:,} with 0 top-{n} meta cats\")\n",
        "\n",
        "# Calculate coverage for science categories\n",
        "print(f\"\\n\" + \"=\" * 80)\n",
        "print(f\"SCIENCE CATEGORY COVERAGE\")\n",
        "print(f\"=\" * 80)\n",
        "science_coverage = calculate_coverage(df, 'science_categories', science_freq, [20, 30, 50, 100])\n",
        "for n, count, pct, zero_top_n in science_coverage:\n",
        "    print(f\"Top {n:3} science cats → {count:5,} articles covered ({pct:5.1f}%) | {zero_top_n:,} with 0 top-{n} science cats\")"
      ],
      "metadata": {
        "id": "I-64D1WNlRcx",
        "outputId": "008edd17-168c-4c5b-fe06-113aac0c8ab3"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": "DATA QUALITY CHECK\n================================================================================\nTotal articles: 16,472\nArticles with 0 languages: 10,735 (65.2%)\n\n================================================================================\nLANGUAGE COVERAGE\n================================================================================\nTop  20 languages  → 5,693 articles covered ( 34.6%) | 44 with 0 top-20 languages\nTop  30 languages  → 5,705 articles covered ( 34.6%) | 32 with 0 top-30 languages\nTop  50 languages  → 5,720 articles covered ( 34.7%) | 17 with 0 top-50 languages\nTop 100 languages  → 5,727 articles covered ( 34.8%) | 10 with 0 top-100 languages\n\n================================================================================\nMETA CATEGORY COVERAGE\n================================================================================\nTop  20 meta cats   → 16,260 articles covered ( 98.7%) | 78 with 0 top-20 meta cats\nTop  30 meta cats   → 16,265 articles covered ( 98.7%) | 73 with 0 top-30 meta cats\nTop  50 meta cats   → 16,291 articles covered ( 98.9%) | 47 with 0 top-50 meta cats\nTop 100 meta cats   → 16,310 articles covered ( 99.0%) | 28 with 0 top-100 meta cats\n\n================================================================================\nSCIENCE CATEGORY COVERAGE\n================================================================================\nTop  20 science cats → 1,709 articles covered ( 10.4%) | 14,711 with 0 top-20 science cats\nTop  30 science cats → 2,105 articles covered ( 12.8%) | 14,315 with 0 top-30 science cats\nTop  50 science cats → 2,820 articles covered ( 17.1%) | 13,600 with 0 top-50 science cats\nTop 100 science cats → 4,267 articles covered ( 25.9%) | 12,153 with 0 top-100 science cats\n"
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "\n",
        "# Find articles with 0 top-20 categories\n",
        "top_20_cats = set([cat for cat, count in cat_freq.most_common(20)])\n",
        "\n",
        "articles_without_top20 = []\n",
        "for idx, row in df.iterrows():\n",
        "    cats = ast.literal_eval(row['all_categories'])\n",
        "    if not any(cat in top_20_cats for cat in cats):\n",
        "        articles_without_top20.append((idx, cats))\n",
        "\n",
        "print(f\"Articles without any top-20 categories: {len(articles_without_top20)}\")\n",
        "\n",
        "# Find all categories in these articles (excluding top 20)\n",
        "remaining_cats_counter = Counter()\n",
        "for idx, cats in articles_without_top20:\n",
        "    for cat in cats:\n",
        "        if cat not in top_20_cats:\n",
        "            remaining_cats_counter[cat] += 1\n",
        "\n",
        "# Greedy algorithm: find minimum set to cover all articles\n",
        "uncovered_articles = set(range(len(articles_without_top20)))\n",
        "selected_categories = []\n",
        "\n",
        "while uncovered_articles:\n",
        "    # Find category that covers the most uncovered articles\n",
        "    best_cat = None\n",
        "    best_coverage = 0\n",
        "\n",
        "    for cat, _ in remaining_cats_counter.items():\n",
        "        if cat in [c for c, _ in selected_categories]:\n",
        "            continue\n",
        "\n",
        "        # Count how many uncovered articles this category would cover\n",
        "        coverage = 0\n",
        "        for i in uncovered_articles:\n",
        "            idx, cats = articles_without_top20[i]\n",
        "            if cat in cats:\n",
        "                coverage += 1\n",
        "\n",
        "        if coverage > best_coverage:\n",
        "            best_coverage = coverage\n",
        "            best_cat = cat\n",
        "\n",
        "    if best_cat is None:\n",
        "        break\n",
        "\n",
        "    # Add this category and mark articles as covered\n",
        "    selected_categories.append((best_cat, best_coverage))\n",
        "    newly_covered = set()\n",
        "    for i in uncovered_articles:\n",
        "        idx, cats = articles_without_top20[i]\n",
        "        if best_cat in cats:\n",
        "            newly_covered.add(i)\n",
        "\n",
        "    uncovered_articles -= newly_covered\n",
        "\n",
        "print(f\"\\n\" + \"=\" * 80)\n",
        "print(f\"MINIMUM CATEGORY SET (beyond top 20)\")\n",
        "print(f\"=\" * 80)\n",
        "print(f\"Categories needed: {len(selected_categories)}\")\n",
        "print(f\"Articles covered: {len(articles_without_top20) - len(uncovered_articles)} / {len(articles_without_top20)}\")\n",
        "print(f\"\\nCategories (in order of coverage):\")\n",
        "for i, (cat, coverage) in enumerate(selected_categories[:15], 1):\n",
        "    print(f\"{i:2}. {cat[:70]:70} → covers {coverage:3} articles\")"
      ],
      "metadata": {
        "id": "Ud-Dn6_nlRcy",
        "outputId": "5ec7ae8e-b52a-4a92-a2a3-b05c6e1e97f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": "Articles without any top-20 categories: 212\n\n================================================================================\nMINIMUM CATEGORY SET (beyond top 20)\n================================================================================\nCategories needed: 131\nArticles covered: 212 / 212\n\nCategories (in order of coverage):\n 1. All articles lacking in-text citations                                 → covers  15 articles\n 2. Economics models                                                       → covers   7 articles\n 3. Wikipedia articles incorporating text from the Congressional Research  → covers   6 articles\n 4. Electronic design                                                      → covers   5 articles\n 5. Economics theorems                                                     → covers   5 articles\n 6. Electric transformers                                                  → covers   5 articles\n 7. Paradoxes in economics                                                 → covers   4 articles\n 8. K-theory                                                               → covers   4 articles\n 9. Wikipedia articles incorporating a citation from the 1911 Encyclopaedi → covers   3 articles\n10. Agriculture                                                            → covers   3 articles\n11. Clinical trials                                                        → covers   3 articles\n12. Meteorological instrumentation and equipment                           → covers   3 articles\n13. Articles with multiple maintenance issues                              → covers   3 articles\n14. International Federation of Vexillological Associations                → covers   3 articles\n15. Articles using infobox templates with no data rows                     → covers   3 articles\n"
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "\n",
        "# Drop unnecessary columns\n",
        "wiki_clean = df.drop(columns=['page_id', 'title', 'url', 'all_categories', 'science_categories']).copy()\n",
        "\n",
        "print(f\"Original columns: {df.shape[1]}\")\n",
        "print(f\"After dropping: {wiki_clean.shape[1]}\")\n",
        "print(f\"\\nRemaining columns:\")\n",
        "print(list(wiki_clean.columns))"
      ],
      "metadata": {
        "id": "gkdbAFjJlRcz",
        "outputId": "3a4b3805-b7b3-4201-dbee-d1ca416365d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": "Original columns: 17\nAfter dropping: 12\n\nRemaining columns:\n['content_length', 'num_links', 'num_sections', 'num_references', 'meta_categories', 'num_categories', 'summary_length', 'avg_sentence_length', 'last_revision_date', 'num_languages', 'languages', 'rating']\n"
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "from collections import Counter\n",
        "\n",
        "# Get top 100 meta categories\n",
        "meta_cats_all = []\n",
        "for idx, row in wiki_clean.iterrows():\n",
        "    meta_c = ast.literal_eval(row['meta_categories'])\n",
        "    meta_cats_all.extend(meta_c)\n",
        "\n",
        "meta_freq = Counter(meta_cats_all)\n",
        "top_100_meta = set([cat for cat, count in meta_freq.most_common(100)])\n",
        "\n",
        "# Filter to articles with at least one top-100 meta category\n",
        "def has_top_meta(row):\n",
        "    meta_c = ast.literal_eval(row['meta_categories'])\n",
        "    return any(cat in top_100_meta for cat in meta_c)\n",
        "\n",
        "wiki_filtered = wiki_clean[wiki_clean.apply(has_top_meta, axis=1)].copy().reset_index(drop=True)\n",
        "\n",
        "print(f\"Before filtering: {len(wiki_clean):,} articles\")\n",
        "print(f\"After filtering:  {len(wiki_filtered):,} articles\")\n",
        "print(f\"Removed:          {len(wiki_clean) - len(wiki_filtered):,} articles ({(len(wiki_clean) - len(wiki_filtered))/len(wiki_clean)*100:.1f}%)\")"
      ],
      "metadata": {
        "id": "-kldi8w4lRcz",
        "outputId": "43068e1e-22c1-4c77-c0de-2c283bc7f198"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": "Before filtering: 16,472 articles\nAfter filtering:  16,310 articles\nRemoved:          162 articles (1.0%)\n"
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "import pandas as pd\n",
        "\n",
        "# Explore rating distribution across meta categories\n",
        "rating_category_data = []\n",
        "\n",
        "for idx, row in wiki_filtered.iterrows():\n",
        "    rating = row['rating']\n",
        "    meta_c = ast.literal_eval(row['meta_categories'])\n",
        "\n",
        "    for cat in meta_c:\n",
        "        if cat in top_100_meta:\n",
        "            rating_category_data.append({'category': cat, 'rating': rating})\n",
        "\n",
        "rating_cat_df = pd.DataFrame(rating_category_data)\n",
        "\n",
        "# Overall rating distribution\n",
        "print(f\"OVERALL RATING DISTRIBUTION\")\n",
        "print(f\"=\" * 80)\n",
        "rating_counts = wiki_filtered['rating'].value_counts().sort_index()\n",
        "for rating, count in rating_counts.items():\n",
        "    pct = count / len(wiki_filtered) * 100\n",
        "    print(f\"{rating:5} → {count:6,} articles ({pct:5.1f}%)\")\n",
        "\n",
        "# Rating distribution by category (top 20 categories)\n",
        "print(f\"\\n\" + \"=\" * 80)\n",
        "print(f\"RATING DISTRIBUTION BY TOP 20 META CATEGORIES\")\n",
        "print(f\"=\" * 80)\n",
        "\n",
        "top_20_cats = [cat for cat, count in meta_freq.most_common(20) if cat in top_100_meta]\n",
        "\n",
        "for cat in top_20_cats[:10]:  # Show first 10 for readability\n",
        "    cat_ratings = rating_cat_df[rating_cat_df['category'] == cat]['rating'].value_counts()\n",
        "    total = cat_ratings.sum()\n",
        "\n",
        "    print(f\"\\n{cat[:70]}\")\n",
        "    print(f\"  Total occurrences: {total:,}\")\n",
        "\n",
        "    rating_dist = []\n",
        "    for rating in ['GA', 'B', 'C', 'START', 'STUB']:\n",
        "        if rating in cat_ratings.index:\n",
        "            count = cat_ratings[rating]\n",
        "            pct = count / total * 100\n",
        "            rating_dist.append(f\"{rating}:{pct:.0f}%\")\n",
        "\n",
        "    print(f\"  Distribution: {', '.join(rating_dist)}\")"
      ],
      "metadata": {
        "id": "cuG0_2pWlRc0",
        "outputId": "ec09efe9-e366-488d-e456-ad058d0658b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": "OVERALL RATING DISTRIBUTION\n================================================================================\nB     →  2,593 articles ( 15.9%)\nC     →  5,834 articles ( 35.8%)\nFA    →    136 articles (  0.8%)\nGA    →    402 articles (  2.5%)\nSTART →  5,829 articles ( 35.7%)\nSTUB  →  1,516 articles (  9.3%)\n\n================================================================================\nRATING DISTRIBUTION BY TOP 20 META CATEGORIES\n================================================================================\n\nArticles with short description\n  Total occurrences: 15,912\n  Distribution: GA:3%, B:16%, C:36%, START:36%, STUB:8%\n\nShort description is different from Wikidata\n  Total occurrences: 10,260\n  Distribution: GA:3%, B:18%, C:38%, START:34%, STUB:5%\n\nShort description matches Wikidata\n  Total occurrences: 5,948\n  Distribution: GA:2%, B:14%, C:32%, START:39%, STUB:12%\n\nAll articles with unsourced statements\n  Total occurrences: 5,751\n  Distribution: GA:1%, B:23%, C:47%, START:27%, STUB:2%\n\nWebarchive template wayback links\n  Total occurrences: 4,990\n  Distribution: GA:4%, B:27%, C:45%, START:21%, STUB:2%\n\nAll articles needing additional references\n  Total occurrences: 3,180\n  Distribution: GA:0%, B:13%, C:40%, START:38%, STUB:8%\n\nCommons category link is on Wikidata\n  Total occurrences: 2,915\n  Distribution: GA:3%, B:23%, C:43%, START:26%, STUB:3%\n\nArticles containing unverified chemical infoboxes\n  Total occurrences: 1,516\n  Distribution: GA:1%, B:17%, C:19%, START:41%, STUB:21%\n\nCommons category link from Wikidata\n  Total occurrences: 1,461\n  Distribution: GA:3%, B:25%, C:46%, START:23%, STUB:2%\n\nAll articles with dead external links\n  Total occurrences: 1,299\n  Distribution: GA:5%, B:29%, C:42%, START:20%, STUB:3%\n"
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "from collections import Counter\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "# Get top 20 languages\n",
        "all_langs = []\n",
        "for idx, row in wiki_filtered.iterrows():\n",
        "    langs = ast.literal_eval(row['languages'])\n",
        "    all_langs.extend(langs)\n",
        "\n",
        "lang_freq = Counter(all_langs)\n",
        "top_20_langs = [lang for lang, count in lang_freq.most_common(20)]\n",
        "\n",
        "print(f\"Top 20 languages: {len(top_20_langs)}\")\n",
        "print(f\"Top 100 meta categories: {len(top_100_meta)}\")\n",
        "print(f\"\\nTop 20 languages:\")\n",
        "for i, lang in enumerate(top_20_langs, 1):\n",
        "    count = lang_freq[lang]\n",
        "    pct = count / len(wiki_filtered) * 100\n",
        "    print(f\"  {i:2}. {lang:5} → {count:5,} ({pct:5.1f}%)\")"
      ],
      "metadata": {
        "id": "T30yH44qlRc0",
        "outputId": "ce8c1416-14b2-41f1-f067-43d60f35be72"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": "Top 20 languages: 20\nTop 100 meta categories: 100\n\nTop 20 languages:\n   1. es    → 4,849 ( 29.7%)\n   2. ar    → 4,825 ( 29.6%)\n   3. fr    → 4,819 ( 29.5%)\n   4. de    → 4,769 ( 29.2%)\n   5. fa    → 4,758 ( 29.2%)\n   6. zh    → 4,567 ( 28.0%)\n   7. ru    → 4,472 ( 27.4%)\n   8. ja    → 4,460 ( 27.3%)\n   9. ca    → 4,376 ( 26.8%)\n  10. pt    → 4,349 ( 26.7%)\n  11. ko    → 4,347 ( 26.7%)\n  12. it    → 4,319 ( 26.5%)\n  13. uk    → 4,204 ( 25.8%)\n  14. pl    → 4,059 ( 24.9%)\n  15. nl    → 4,046 ( 24.8%)\n  16. cs    → 3,935 ( 24.1%)\n  17. id    → 3,843 ( 23.6%)\n  18. fi    → 3,752 ( 23.0%)\n  19. sv    → 3,619 ( 22.2%)\n  20. tr    → 3,554 ( 21.8%)\n"
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "def filter_and_encode(df, column_name, top_items, prefix):\n",
        "    \"\"\"\n",
        "    Filter items to top-N list and one-hot encode them.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame with the column to encode\n",
        "    - column_name: Name of column containing list-like strings\n",
        "    - top_items: Set or list of items to keep\n",
        "    - prefix: Prefix for encoded column names\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame with binary encoded columns\n",
        "    \"\"\"\n",
        "    # Parse and filter to top items only\n",
        "    filtered_lists = []\n",
        "    for idx, row in df.iterrows():\n",
        "        items = ast.literal_eval(row[column_name])\n",
        "        filtered = [item for item in items if item in top_items]\n",
        "        filtered_lists.append(filtered)\n",
        "\n",
        "    # One-hot encode\n",
        "    mlb = MultiLabelBinarizer()\n",
        "    encoded = mlb.fit_transform(filtered_lists)\n",
        "\n",
        "    # Create DataFrame with proper column names\n",
        "    encoded_df = pd.DataFrame(\n",
        "        encoded,\n",
        "        columns=[f'{prefix}_{item}' for item in mlb.classes_],\n",
        "        index=df.index\n",
        "    )\n",
        "\n",
        "    return encoded_df\n",
        "\n",
        "print(\"Function defined: filter_and_encode()\")\n",
        "print(\"  - Filters items to top-N list\")\n",
        "print(\"  - One-hot encodes filtered items\")\n",
        "print(\"  - Returns DataFrame with binary columns\")"
      ],
      "metadata": {
        "id": "31cH-EGvlRc0",
        "outputId": "da9f44e9-7421-488c-bb3c-e6beb5e1ca72"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": "Function defined: filter_and_encode()\n  - Filters items to top-N list\n  - One-hot encodes filtered items\n  - Returns DataFrame with binary columns\n"
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode meta categories (top 100)\n",
        "meta_encoded = filter_and_encode(wiki_filtered, 'meta_categories', top_100_meta, 'meta')\n",
        "\n",
        "# Encode languages (top 20)\n",
        "lang_encoded = filter_and_encode(wiki_filtered, 'languages', top_20_langs, 'lang')\n",
        "\n",
        "print(f\"Encoding complete:\")\n",
        "print(f\"  Meta categories: {meta_encoded.shape[1]} binary features\")\n",
        "print(f\"  Languages:       {lang_encoded.shape[1]} binary features\")\n",
        "print(f\"  Total new features: {meta_encoded.shape[1] + lang_encoded.shape[1]}\")"
      ],
      "metadata": {
        "id": "qNfx6TxwlRc0",
        "outputId": "9283cd7b-2c32-4df9-9659-52118799bfbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": "Encoding complete:\n  Meta categories: 100 binary features\n  Languages:       20 binary features\n  Total new features: 120\n"
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all features into final preprocessed dataset\n",
        "wiki_preprocessed = pd.concat([\n",
        "    wiki_filtered.drop(['meta_categories', 'languages'], axis=1),\n",
        "    meta_encoded,\n",
        "    lang_encoded\n",
        "], axis=1)\n",
        "\n",
        "print(f\"FINAL PREPROCESSED DATASET\")\n",
        "print(f\"=\" * 80)\n",
        "print(f\"Shape: {wiki_preprocessed.shape}\")\n",
        "print(f\"  Articles: {wiki_preprocessed.shape[0]:,}\")\n",
        "print(f\"  Features: {wiki_preprocessed.shape[1]:,}\")\n",
        "print(f\"\\nFeature breakdown:\")\n",
        "print(f\"  Original features: {len(wiki_filtered.drop(['meta_categories', 'languages'], axis=1).columns)}\")\n",
        "print(f\"  Meta category features: {meta_encoded.shape[1]}\")\n",
        "print(f\"  Language features: {lang_encoded.shape[1]}\")\n",
        "print(f\"\\nColumn types:\")\n",
        "print(f\"  rating (target)\")\n",
        "print(f\"  content_length, num_links, num_images, num_refs, num_sections (numerical)\")\n",
        "print(f\"  meta_* (100 binary category features)\")\n",
        "print(f\"  lang_* (20 binary language features)\")\n",
        "\n",
        "wiki_preprocessed.head()"
      ],
      "metadata": {
        "id": "nZfAAaOClRc0",
        "outputId": "91a2d968-46de-4ac0-caab-472023693d5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": "FINAL PREPROCESSED DATASET\n================================================================================\nShape: (16310, 130)\n  Articles: 16,310\n  Features: 130\n\nFeature breakdown:\n  Original features: 10\n  Meta category features: 100\n  Language features: 20\n\nColumn types:\n  rating (target)\n  content_length, num_links, num_images, num_refs, num_sections (numerical)\n  meta_* (100 binary category features)\n  lang_* (20 binary language features)\n"
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "application/vnd.hex.export+parquet": {
              "success": true,
              "exportKey": "019bae66-d350-7001-a251-d171be1cf243/019be885-c92a-7aac-b4fd-d91fd28ebe86/exports/019bec8a-159e-7000-880e-2b25b9c67c3c"
            },
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>content_length</th>\n      <th>num_links</th>\n      <th>num_sections</th>\n      <th>num_references</th>\n      <th>num_categories</th>\n      <th>summary_length</th>\n      <th>avg_sentence_length</th>\n      <th>last_revision_date</th>\n      <th>num_languages</th>\n      <th>rating</th>\n      <th>...</th>\n      <th>lang_ja</th>\n      <th>lang_ko</th>\n      <th>lang_nl</th>\n      <th>lang_pl</th>\n      <th>lang_pt</th>\n      <th>lang_ru</th>\n      <th>lang_sv</th>\n      <th>lang_tr</th>\n      <th>lang_uk</th>\n      <th>lang_zh</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>49731</td>\n      <td>184</td>\n      <td>16</td>\n      <td>138</td>\n      <td>20</td>\n      <td>2198</td>\n      <td>24.40</td>\n      <td>2025-11-25T20:44:17Z</td>\n      <td>92</td>\n      <td>GA</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>24708</td>\n      <td>265</td>\n      <td>10</td>\n      <td>43</td>\n      <td>9</td>\n      <td>698</td>\n      <td>25.20</td>\n      <td>2025-12-01T13:21:16Z</td>\n      <td>9</td>\n      <td>C</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>49610</td>\n      <td>51</td>\n      <td>24</td>\n      <td>65</td>\n      <td>19</td>\n      <td>273</td>\n      <td>20.50</td>\n      <td>2026-01-04T15:47:15Z</td>\n      <td>24</td>\n      <td>C</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>19934</td>\n      <td>0</td>\n      <td>12</td>\n      <td>32</td>\n      <td>25</td>\n      <td>369</td>\n      <td>25.00</td>\n      <td>2026-01-08T01:18:38Z</td>\n      <td>26</td>\n      <td>B</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>62518</td>\n      <td>0</td>\n      <td>18</td>\n      <td>60</td>\n      <td>28</td>\n      <td>841</td>\n      <td>20.67</td>\n      <td>2025-12-27T19:49:48Z</td>\n      <td>18</td>\n      <td>B</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 130 columns</p>\n</div>"
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_preprocessed.to_csv(\"wiki_preprocessed.csv\", index=False)"
      ],
      "metadata": {
        "id": "X0QUWXjzlRc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split: 70% train, 15% validation, 15% test\n",
        "# First split: separate out test set\n",
        "train_val, test = train_test_split(\n",
        "    wiki_preprocessed,\n",
        "    test_size=0.15,\n",
        "    random_state=42,\n",
        "    stratify=wiki_preprocessed['rating']\n",
        ")\n",
        "\n",
        "# Second split: separate train from validation\n",
        "train, validation = train_test_split(\n",
        "    train_val,\n",
        "    test_size=0.176,  # 0.176 * 0.85 ≈ 0.15 of total\n",
        "    random_state=42,\n",
        "    stratify=train_val['rating']\n",
        ")\n",
        "\n",
        "# Save to CSV files\n",
        "train.to_csv('train.csv', index=False)\n",
        "validation.to_csv('validation.csv', index=False)\n",
        "test.to_csv('test.csv', index=False)\n",
        "\n",
        "print(f\"DATA SPLIT COMPLETE\")\n",
        "print(f\"=\" * 80)\n",
        "print(f\"Total articles: {len(wiki_preprocessed):,}\")\n",
        "print(f\"\\nSplit sizes:\")\n",
        "print(f\"  Train:      {len(train):6,} ({len(train)/len(wiki_preprocessed)*100:5.1f}%)\")\n",
        "print(f\"  Validation: {len(validation):6,} ({len(validation)/len(wiki_preprocessed)*100:5.1f}%)\")\n",
        "print(f\"  Test:       {len(test):6,} ({len(test)/len(wiki_preprocessed)*100:5.1f}%)\")\n",
        "\n",
        "print(f\"\\nRating distribution (train):\")\n",
        "for rating, count in train['rating'].value_counts().sort_index().items():\n",
        "    pct = count / len(train) * 100\n",
        "    print(f\"  {rating:5} → {count:5,} ({pct:5.1f}%)\")\n",
        "\n",
        "print(f\"\\nFiles saved:\")\n",
        "print(f\"  train.csv\")\n",
        "print(f\"  validation.csv\")\n",
        "print(f\"  test.csv\")"
      ],
      "metadata": {
        "id": "JWFhJmgAlRc1",
        "outputId": "a6d34bf4-7d8e-4694-9bfd-ed04c3e30eba"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": "DATA SPLIT COMPLETE\n================================================================================\nTotal articles: 16,310\n\nSplit sizes:\n  Train:      11,423 ( 70.0%)\n  Validation:  2,440 ( 15.0%)\n  Test:        2,447 ( 15.0%)\n\nRating distribution (train):\n  B     → 1,816 ( 15.9%)\n  C     → 4,086 ( 35.8%)\n  FA    →    96 (  0.8%)\n  GA    →   282 (  2.5%)\n  START → 4,082 ( 35.7%)\n  STUB  → 1,061 (  9.3%)\n\nFiles saved:\n  train.csv\n  validation.csv\n  test.csv\n"
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    }
  ],
  "metadata": {
    "orig_nbformat": 4,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "hex_info": {
      "author": "Lucas Campagnaro",
      "project_id": "019be885-c92a-7aac-b4fd-d91fd28ebe86",
      "version": "draft",
      "exported_date": "Fri Jan 23 2026 20:38:26 GMT+0000 (Coordinated Universal Time)"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}