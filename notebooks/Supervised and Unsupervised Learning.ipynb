{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as _hex_pandas\n",
        "import datetime as _hex_datetime\n",
        "import json as _hex_json"
      ],
      "execution_count": null,
      "metadata": {
        "id": "PCdGytVKxaC_"
      },
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hex_scheduled = _hex_json.loads(\"false\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "FueAV4VaxaDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hex_user_email = _hex_json.loads(\"\\\"example-user@example.com\\\"\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "N0lGgSH8xaDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hex_user_attributes = _hex_json.loads(\"{}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "sBYc6WXfxaDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hex_run_context = _hex_json.loads(\"\\\"logic\\\"\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "hsQ37FEvxaDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hex_timezone = _hex_json.loads(\"\\\"UTC\\\"\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "nhA19RjyxaDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hex_project_id = _hex_json.loads(\"\\\"019be885-c92a-7aac-b4fd-d91fd28ebe86\\\"\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "G9-i15yIxaDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hex_project_name = _hex_json.loads(\"\\\"Supervised and Unsupervised Learning\\\"\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "gkvLsc3AxaDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hex_status = _hex_json.loads(\"\\\"\\\"\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "zvnJ4kUUxaDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hex_categories = _hex_json.loads(\"[]\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "D-8f9X69xaDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hex_color_palette = _hex_json.loads(\"[\\\"#4C78A8\\\",\\\"#F58518\\\",\\\"#E45756\\\",\\\"#72B7B2\\\",\\\"#54A24B\\\",\\\"#EECA3B\\\",\\\"#B279A2\\\",\\\"#FF9DA6\\\",\\\"#9D755D\\\",\\\"#BAB0AC\\\"]\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "SndJMt08xaDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('wikipedia_dataset.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "0qENrKTSxaDI",
        "outputId": "42c2bb30-81a6-4f99-c546-de0dea499eac"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.hex.export+parquet": {
              "success": true,
              "exportKey": "019bae66-d350-7001-a251-d171be1cf243/019be885-c92a-7aac-b4fd-d91fd28ebe86/exports/019be8b3-0b51-7dd3-9d1c-d63f08891e24"
            },
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>page_id</th>\n      <th>title</th>\n      <th>url</th>\n      <th>content_length</th>\n      <th>num_links</th>\n      <th>num_sections</th>\n      <th>num_references</th>\n      <th>all_categories</th>\n      <th>meta_categories</th>\n      <th>science_categories</th>\n      <th>num_categories</th>\n      <th>summary_length</th>\n      <th>avg_sentence_length</th>\n      <th>last_revision_date</th>\n      <th>num_languages</th>\n      <th>languages</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>47262</td>\n      <td>2 Pallas</td>\n      <td>https://en.wikipedia.org/wiki/2_Pallas</td>\n      <td>49731</td>\n      <td>184</td>\n      <td>16</td>\n      <td>138</td>\n      <td>['1802 in science', 'Articles containing Ancie...</td>\n      <td>[]</td>\n      <td>['1802 in science', 'Articles containing Ancie...</td>\n      <td>20</td>\n      <td>2198</td>\n      <td>24.40</td>\n      <td>2025-11-25T20:44:17Z</td>\n      <td>92</td>\n      <td>['af', 'gsw', 'an', 'anp', 'ar', 'arz', 'ast',...</td>\n      <td>GA</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>71318</td>\n      <td>100-year flood</td>\n      <td>https://en.wikipedia.org/wiki/100-year_flood</td>\n      <td>24708</td>\n      <td>265</td>\n      <td>10</td>\n      <td>43</td>\n      <td>['Actuarial science', 'Articles with short des...</td>\n      <td>[]</td>\n      <td>['Actuarial science', 'Articles with short des...</td>\n      <td>9</td>\n      <td>698</td>\n      <td>25.20</td>\n      <td>2025-12-01T13:21:16Z</td>\n      <td>9</td>\n      <td>['ar', 'de', 'es', 'fa', 'fr', 'ms', 'nb', 'sv...</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>175149</td>\n      <td>'Pataphysics</td>\n      <td>https://en.wikipedia.org/wiki/%27Pataphysics</td>\n      <td>49610</td>\n      <td>51</td>\n      <td>24</td>\n      <td>65</td>\n      <td>[\"'Pataphysics\", 'Alfred Jarry', 'All articles...</td>\n      <td>[]</td>\n      <td>[\"'Pataphysics\", 'Alfred Jarry', 'All articles...</td>\n      <td>19</td>\n      <td>273</td>\n      <td>20.50</td>\n      <td>2026-01-04T15:47:15Z</td>\n      <td>24</td>\n      <td>['ast', 'bg', 'ca', 'cs', 'de', 'el', 'es', 'e...</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>786020</td>\n      <td>1,4-Butanediol</td>\n      <td>https://en.wikipedia.org/wiki/1,4-Butanediol</td>\n      <td>19934</td>\n      <td>0</td>\n      <td>12</td>\n      <td>32</td>\n      <td>['1,4-Butanediyl compounds', 'Alcohol solvents...</td>\n      <td>[]</td>\n      <td>['1,4-Butanediyl compounds', 'Alcohol solvents...</td>\n      <td>25</td>\n      <td>369</td>\n      <td>25.00</td>\n      <td>2026-01-08T01:18:38Z</td>\n      <td>26</td>\n      <td>['ar', 'az', 'azb', 'ca', 'de', 'eo', 'es', 'e...</td>\n      <td>B</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1110853</td>\n      <td>*Dyēus</td>\n      <td>https://en.wikipedia.org/wiki/*Dy%C4%93us</td>\n      <td>62518</td>\n      <td>0</td>\n      <td>18</td>\n      <td>60</td>\n      <td>['Articles containing Albanian-language text',...</td>\n      <td>[]</td>\n      <td>['Articles containing Albanian-language text',...</td>\n      <td>28</td>\n      <td>839</td>\n      <td>20.67</td>\n      <td>2025-12-27T19:49:48Z</td>\n      <td>18</td>\n      <td>['ar', 'ca', 'cs', 'es', 'ext', 'fa', 'fi', 'f...</td>\n      <td>B</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>16467</th>\n      <td>5065997</td>\n      <td>Zirconium(IV) silicate</td>\n      <td>https://en.wikipedia.org/wiki/Zirconium(IV)_si...</td>\n      <td>6474</td>\n      <td>102</td>\n      <td>5</td>\n      <td>9</td>\n      <td>['Articles containing unverified chemical info...</td>\n      <td>[]</td>\n      <td>['Articles containing unverified chemical info...</td>\n      <td>16</td>\n      <td>391</td>\n      <td>8.57</td>\n      <td>2025-09-24T00:32:38Z</td>\n      <td>14</td>\n      <td>['ar', 'azb', 'cs', 'de', 'es', 'fa', 'fr', 'i...</td>\n      <td>START</td>\n    </tr>\n    <tr>\n      <th>16468</th>\n      <td>24215517</td>\n      <td>Zirconium(IV) sulfate</td>\n      <td>https://en.wikipedia.org/wiki/Zirconium(IV)_su...</td>\n      <td>4083</td>\n      <td>346</td>\n      <td>3</td>\n      <td>4</td>\n      <td>['Articles containing unverified chemical info...</td>\n      <td>[]</td>\n      <td>['Articles containing unverified chemical info...</td>\n      <td>14</td>\n      <td>354</td>\n      <td>15.00</td>\n      <td>2025-06-24T01:25:10Z</td>\n      <td>14</td>\n      <td>['ar', 'azb', 'bn', 'de', 'fa', 'fi', 'fr', 'i...</td>\n      <td>STUB</td>\n    </tr>\n    <tr>\n      <th>16469</th>\n      <td>34411</td>\n      <td>Zodiac</td>\n      <td>https://en.wikipedia.org/wiki/Zodiac</td>\n      <td>67343</td>\n      <td>500</td>\n      <td>18</td>\n      <td>99</td>\n      <td>['All Wikipedia articles written in American E...</td>\n      <td>[]</td>\n      <td>['All Wikipedia articles written in American E...</td>\n      <td>32</td>\n      <td>1703</td>\n      <td>24.91</td>\n      <td>2025-12-17T05:15:17Z</td>\n      <td>107</td>\n      <td>['af', 'ar', 'arz', 'ast', 'az', 'ba', 'bcl', ...</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>16470</th>\n      <td>34527</td>\n      <td>Zodiacal light</td>\n      <td>https://en.wikipedia.org/wiki/Zodiacal_light</td>\n      <td>22376</td>\n      <td>337</td>\n      <td>10</td>\n      <td>35</td>\n      <td>['All Wikipedia articles in need of updating',...</td>\n      <td>[]</td>\n      <td>['All Wikipedia articles in need of updating',...</td>\n      <td>11</td>\n      <td>1697</td>\n      <td>24.82</td>\n      <td>2025-08-10T18:14:11Z</td>\n      <td>55</td>\n      <td>['af', 'ar', 'ary', 'az', 'bg', 'ca', 'ckb', '...</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>16471</th>\n      <td>351466</td>\n      <td>Zonda wind</td>\n      <td>https://en.wikipedia.org/wiki/Zonda_wind</td>\n      <td>2868</td>\n      <td>21</td>\n      <td>5</td>\n      <td>0</td>\n      <td>['All articles needing coordinates', 'Argentin...</td>\n      <td>[]</td>\n      <td>['All articles needing coordinates', 'Argentin...</td>\n      <td>11</td>\n      <td>139</td>\n      <td>25.00</td>\n      <td>2025-07-27T06:43:28Z</td>\n      <td>19</td>\n      <td>['ar', 'be', 'bh', 'ca', 'de', 'es', 'fr', 'ha...</td>\n      <td>START</td>\n    </tr>\n  </tbody>\n</table>\n<p>16472 rows × 17 columns</p>\n</div>"
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "from collections import Counter\n",
        "\n",
        "# Parse all lists to count unique values\n",
        "all_langs = []\n",
        "all_cats = []\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    langs = ast.literal_eval(row['languages'])\n",
        "    all_langs.extend(langs)\n",
        "\n",
        "    cats = ast.literal_eval(row['all_categories'])\n",
        "    all_cats.extend(cats)\n",
        "\n",
        "# Count uniques\n",
        "unique_langs = set(all_langs)\n",
        "unique_cats = set(all_cats)\n",
        "lang_freq = Counter(all_langs)\n",
        "cat_freq = Counter(all_cats)\n",
        "\n",
        "print(f\"DATASET OVERVIEW\")\n",
        "print(f\"=\" * 80)\n",
        "print(f\"Total articles: {len(df):,}\")\n",
        "print(f\"Unique languages: {len(unique_langs):,}\")\n",
        "print(f\"Unique categories: {len(unique_cats):,}\")\n",
        "print(f\"\\nPotential one-hot encoded features:\")\n",
        "print(f\"  Languages: {len(unique_langs):,} binary columns\")\n",
        "print(f\"  Categories: {len(unique_cats):,} binary columns\")\n",
        "print(f\"  TOTAL: {len(unique_langs) + len(unique_cats):,} new features\")\n",
        "\n",
        "print(f\"\\n\" + \"=\" * 80)\n",
        "print(f\"LANGUAGE DISTRIBUTION\")\n",
        "print(f\"=\" * 80)\n",
        "print(f\"Top 20 most common languages:\")\n",
        "for lang, count in lang_freq.most_common(20):\n",
        "    pct = count / len(df) * 100\n",
        "    print(f\"  {lang:5} → {count:3} articles ({pct:5.1f}%)\")\n",
        "\n",
        "print(f\"\\n\" + \"=\" * 80)\n",
        "print(f\"CATEGORY DISTRIBUTION\")\n",
        "print(f\"=\" * 80)\n",
        "print(f\"Top 30 most common categories:\")\n",
        "for cat, count in cat_freq.most_common(30):\n",
        "    pct = count / len(df) * 100\n",
        "    print(f\"  {cat[:60]:60} → {count:3} articles ({pct:5.1f}%)\")"
      ],
      "metadata": {
        "id": "fes3DiBxxaDI",
        "outputId": "930c1194-bf94-42b0-ab3f-57c95485b802"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": "DATASET OVERVIEW\n================================================================================\nTotal articles: 16,472\nUnique languages: 342\nUnique categories: 22,885\n\nPotential one-hot encoded features:\n  Languages: 342 binary columns\n  Categories: 22,885 binary columns\n  TOTAL: 23,227 new features\n\n================================================================================\nLANGUAGE DISTRIBUTION\n================================================================================\nTop 20 most common languages:\n  es    → 4852 articles ( 29.5%)\n  ar    → 4825 articles ( 29.3%)\n  fr    → 4824 articles ( 29.3%)\n  de    → 4776 articles ( 29.0%)\n  fa    → 4760 articles ( 28.9%)\n  zh    → 4570 articles ( 27.7%)\n  ru    → 4472 articles ( 27.1%)\n  ja    → 4464 articles ( 27.1%)\n  ca    → 4374 articles ( 26.6%)\n  pt    → 4350 articles ( 26.4%)\n  ko    → 4346 articles ( 26.4%)\n  it    → 4322 articles ( 26.2%)\n  uk    → 4209 articles ( 25.6%)\n  pl    → 4060 articles ( 24.6%)\n  nl    → 4045 articles ( 24.6%)\n  cs    → 3935 articles ( 23.9%)\n  id    → 3842 articles ( 23.3%)\n  fi    → 3750 articles ( 22.8%)\n  sv    → 3617 articles ( 22.0%)\n  tr    → 3554 articles ( 21.6%)\n\n================================================================================\nCATEGORY DISTRIBUTION\n================================================================================\nTop 30 most common categories:\n  Articles with short description                              → 15904 articles ( 96.6%)\n  Short description is different from Wikidata                 → 10248 articles ( 62.2%)\n  Short description matches Wikidata                           → 5956 articles ( 36.2%)\n  All articles with unsourced statements                       → 5760 articles ( 35.0%)\n  Webarchive template wayback links                            → 4978 articles ( 30.2%)\n  All articles needing additional references                   → 3177 articles ( 19.3%)\n  Commons category link is on Wikidata                         → 2917 articles ( 17.7%)\n  Articles containing unverified chemical infoboxes            → 1515 articles (  9.2%)\n  Commons category link from Wikidata                          → 1460 articles (  8.9%)\n  All articles with dead external links                        → 1301 articles (  7.9%)\n  ECHA InfoCard ID from Wikidata                               → 1279 articles (  7.8%)\n  CS1: long volume value                                       → 1223 articles (  7.4%)\n  CS1 maint: multiple names: authors list                      → 1189 articles (  7.2%)\n  Articles without KEGG source                                 → 1119 articles (  6.8%)\n  CS1 German-language sources (de)                             → 1117 articles (  6.8%)\n  CS1 errors: ISBN date                                        → 1080 articles (  6.6%)\n  All stub articles                                            → 1038 articles (  6.3%)\n  All Wikipedia articles written in American English           → 947 articles (  5.7%)\n  Articles with permanently dead external links                → 890 articles (  5.4%)\n  Articles with excerpts                                       → 890 articles (  5.4%)\n  Articles containing Latin-language text                      → 854 articles (  5.2%)\n  All articles containing potentially dated statements         → 834 articles (  5.1%)\n  CS1: unfit URL                                               → 777 articles (  4.7%)\n  CS1 French-language sources (fr)                             → 766 articles (  4.7%)\n  Chembox having GHS data                                      → 760 articles (  4.6%)\n  Articles without EBI source                                  → 729 articles (  4.4%)\n  CS1 maint: work parameter with ISBN                          → 704 articles (  4.3%)\n  CS1 maint: location missing publisher                        → 685 articles (  4.2%)\n  Chembox image size set                                       → 652 articles (  4.0%)\n  All articles lacking reliable references                     → 633 articles (  3.8%)\n"
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "\n",
        "# Calculate coverage for top-N categories and languages\n",
        "def calculate_coverage(df, column_name, freq_counter, top_n_list):\n",
        "    \"\"\"Calculate % of articles covered by top-N items\"\"\"\n",
        "    results = []\n",
        "\n",
        "    for n in top_n_list:\n",
        "        top_n_items = set([item for item, count in freq_counter.most_common(n)])\n",
        "\n",
        "        articles_covered = 0\n",
        "        articles_with_zero_top_n = 0\n",
        "\n",
        "        for idx, row in df.iterrows():\n",
        "            items = ast.literal_eval(row[column_name])\n",
        "\n",
        "            # Check if article has at least one of the top-N items\n",
        "            if any(item in top_n_items for item in items):\n",
        "                articles_covered += 1\n",
        "            elif len(items) > 0:  # Has items but none are in top-N\n",
        "                articles_with_zero_top_n += 1\n",
        "\n",
        "        coverage_pct = articles_covered / len(df) * 100\n",
        "        results.append((n, articles_covered, coverage_pct, articles_with_zero_top_n))\n",
        "\n",
        "    return results\n",
        "\n",
        "# Check for articles with 0 languages\n",
        "articles_with_zero_langs = 0\n",
        "for idx, row in df.iterrows():\n",
        "    langs = ast.literal_eval(row['languages'])\n",
        "    if len(langs) == 0:\n",
        "        articles_with_zero_langs += 1\n",
        "\n",
        "print(f\"DATA QUALITY CHECK\")\n",
        "print(f\"=\" * 80)\n",
        "print(f\"Total articles: {len(df):,}\")\n",
        "print(f\"Articles with 0 languages: {articles_with_zero_langs:,} ({articles_with_zero_langs/len(df)*100:.1f}%)\")\n",
        "\n",
        "# Calculate coverage for categories\n",
        "print(f\"\\n\" + \"=\" * 80)\n",
        "print(f\"CATEGORY COVERAGE\")\n",
        "print(f\"=\" * 80)\n",
        "cat_coverage = calculate_coverage(df, 'all_categories', cat_freq, [5, 10, 20, 30, 50, 100])\n",
        "for n, count, pct, zero_top_n in cat_coverage:\n",
        "    print(f\"Top {n:3} categories → {count:5,} articles covered ({pct:5.1f}%) | {zero_top_n:,} with 0 top-{n} categories\")\n",
        "\n",
        "# Calculate coverage for languages\n",
        "print(f\"\\n\" + \"=\" * 80)\n",
        "print(f\"LANGUAGE COVERAGE\")\n",
        "print(f\"=\" * 80)\n",
        "lang_coverage = calculate_coverage(df, 'languages', lang_freq, [20, 30, 50, 100])\n",
        "for n, count, pct, zero_top_n in lang_coverage:\n",
        "    print(f\"Top {n:3} languages  → {count:5,} articles covered ({pct:5.1f}%) | {zero_top_n:,} with 0 top-{n} languages\")"
      ],
      "metadata": {
        "id": "AdlFtQ8YxaDJ",
        "outputId": "34525119-8593-4420-a19d-26d1afc1ddfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": "DATA QUALITY CHECK\n================================================================================\nTotal articles: 16,472\nArticles with 0 languages: 10,738 (65.2%)\n\n================================================================================\nCATEGORY COVERAGE\n================================================================================\nTop   5 categories → 15,985 articles covered ( 97.0%) | 487 with 0 top-5 categories\nTop  10 categories → 16,087 articles covered ( 97.7%) | 385 with 0 top-10 categories\nTop  20 categories → 16,260 articles covered ( 98.7%) | 212 with 0 top-20 categories\nTop  30 categories → 16,264 articles covered ( 98.7%) | 208 with 0 top-30 categories\nTop  50 categories → 16,290 articles covered ( 98.9%) | 182 with 0 top-50 categories\nTop 100 categories → 16,311 articles covered ( 99.0%) | 161 with 0 top-100 categories\n\n================================================================================\nLANGUAGE COVERAGE\n================================================================================\nTop  20 languages  → 5,690 articles covered ( 34.5%) | 44 with 0 top-20 languages\nTop  30 languages  → 5,702 articles covered ( 34.6%) | 32 with 0 top-30 languages\nTop  50 languages  → 5,717 articles covered ( 34.7%) | 17 with 0 top-50 languages\nTop 100 languages  → 5,724 articles covered ( 34.7%) | 10 with 0 top-100 languages\n"
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "\n",
        "# Find articles with 0 top-20 categories\n",
        "top_20_cats = set([cat for cat, count in cat_freq.most_common(20)])\n",
        "\n",
        "articles_without_top20 = []\n",
        "for idx, row in df.iterrows():\n",
        "    cats = ast.literal_eval(row['all_categories'])\n",
        "    if not any(cat in top_20_cats for cat in cats):\n",
        "        articles_without_top20.append((idx, cats))\n",
        "\n",
        "print(f\"Articles without any top-20 categories: {len(articles_without_top20)}\")\n",
        "\n",
        "# Find all categories in these articles (excluding top 20)\n",
        "remaining_cats_counter = Counter()\n",
        "for idx, cats in articles_without_top20:\n",
        "    for cat in cats:\n",
        "        if cat not in top_20_cats:\n",
        "            remaining_cats_counter[cat] += 1\n",
        "\n",
        "# Greedy algorithm: find minimum set to cover all articles\n",
        "uncovered_articles = set(range(len(articles_without_top20)))\n",
        "selected_categories = []\n",
        "\n",
        "while uncovered_articles:\n",
        "    # Find category that covers the most uncovered articles\n",
        "    best_cat = None\n",
        "    best_coverage = 0\n",
        "\n",
        "    for cat, _ in remaining_cats_counter.items():\n",
        "        if cat in [c for c, _ in selected_categories]:\n",
        "            continue\n",
        "\n",
        "        # Count how many uncovered articles this category would cover\n",
        "        coverage = 0\n",
        "        for i in uncovered_articles:\n",
        "            idx, cats = articles_without_top20[i]\n",
        "            if cat in cats:\n",
        "                coverage += 1\n",
        "\n",
        "        if coverage > best_coverage:\n",
        "            best_coverage = coverage\n",
        "            best_cat = cat\n",
        "\n",
        "    if best_cat is None:\n",
        "        break\n",
        "\n",
        "    # Add this category and mark articles as covered\n",
        "    selected_categories.append((best_cat, best_coverage))\n",
        "    newly_covered = set()\n",
        "    for i in uncovered_articles:\n",
        "        idx, cats = articles_without_top20[i]\n",
        "        if best_cat in cats:\n",
        "            newly_covered.add(i)\n",
        "\n",
        "    uncovered_articles -= newly_covered\n",
        "\n",
        "print(f\"\\n\" + \"=\" * 80)\n",
        "print(f\"MINIMUM CATEGORY SET (beyond top 20)\")\n",
        "print(f\"=\" * 80)\n",
        "print(f\"Categories needed: {len(selected_categories)}\")\n",
        "print(f\"Articles covered: {len(articles_without_top20) - len(uncovered_articles)} / {len(articles_without_top20)}\")\n",
        "print(f\"\\nCategories (in order of coverage):\")\n",
        "for i, (cat, coverage) in enumerate(selected_categories[:15], 1):\n",
        "    print(f\"{i:2}. {cat[:70]:70} → covers {coverage:3} articles\")"
      ],
      "metadata": {
        "id": "8VZMLlBZxaDJ",
        "outputId": "54e454fa-eb6e-481d-e233-c6b5910ebd89"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": "Articles without any top-20 categories: 212\n\n================================================================================\nMINIMUM CATEGORY SET (beyond top 20)\n================================================================================\nCategories needed: 130\nArticles covered: 212 / 212\n\nCategories (in order of coverage):\n 1. All articles lacking in-text citations                                 → covers  15 articles\n 2. Economics models                                                       → covers   7 articles\n 3. Wikipedia articles incorporating text from the Congressional Research  → covers   6 articles\n 4. Electronic design                                                      → covers   5 articles\n 5. Economics theorems                                                     → covers   5 articles\n 6. Electric transformers                                                  → covers   5 articles\n 7. Paradoxes in economics                                                 → covers   4 articles\n 8. K-theory                                                               → covers   4 articles\n 9. Structural engineering                                                 → covers   3 articles\n10. Wikipedia articles incorporating a citation from the 1911 Encyclopaedi → covers   3 articles\n11. Severe weather and convection                                          → covers   3 articles\n12. Agriculture                                                            → covers   3 articles\n13. Clinical trials                                                        → covers   3 articles\n14. Meteorological instrumentation and equipment                           → covers   3 articles\n15. Articles with multiple maintenance issues                              → covers   3 articles\n"
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "# Parse the string representations of lists\n",
        "wiki_preprocessed = wiki_data.copy()\n",
        "\n",
        "# Convert string representations to actual lists\n",
        "wiki_preprocessed['languages'] = wiki_preprocessed['languages'].apply(ast.literal_eval)\n",
        "wiki_preprocessed['all_categories'] = wiki_preprocessed['all_categories'].apply(ast.literal_eval)\n",
        "\n",
        "# One-hot encode languages\n",
        "mlb_languages = MultiLabelBinarizer()\n",
        "languages_encoded = mlb_languages.fit_transform(wiki_preprocessed['languages'])\n",
        "languages_df = pd.DataFrame(\n",
        "    languages_encoded,\n",
        "    columns=[f'lang_{lang}' for lang in mlb_languages.classes_],\n",
        "    index=wiki_preprocessed.index\n",
        ")\n",
        "\n",
        "# One-hot encode categories\n",
        "mlb_categories = MultiLabelBinarizer()\n",
        "categories_encoded = mlb_categories.fit_transform(wiki_preprocessed['all_categories'])\n",
        "categories_df = pd.DataFrame(\n",
        "    categories_encoded,\n",
        "    columns=[f'cat_{cat}' for cat in mlb_categories.classes_],\n",
        "    index=wiki_preprocessed.index\n",
        ")\n",
        "\n",
        "# Combine everything\n",
        "wiki_preprocessed = pd.concat([\n",
        "    wiki_preprocessed.drop(['languages', 'all_categories', 'meta_categories', 'science_categories'], axis=1),\n",
        "    languages_df,\n",
        "    categories_df\n",
        "], axis=1)\n",
        "\n",
        "print(f\"Original shape: {wiki_data.shape}\")\n",
        "print(f\"Preprocessed shape: {wiki_preprocessed.shape}\")\n",
        "print(f\"\\nNew columns added:\")\n",
        "print(f\"- Language features: {len(languages_df.columns)}\")\n",
        "print(f\"- Category features: {len(categories_df.columns)}\")\n",
        "print(f\"- Total features: {wiki_preprocessed.shape[1]}\")\n",
        "\n",
        "wiki_preprocessed.head()"
      ],
      "metadata": {
        "id": "-6KhWIyKxaDJ",
        "outputId": "0aa0a10e-47b5-4f65-cd5a-a4cfdf584442"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": "Original shape: (10, 17)\nPreprocessed shape: (10, 222)\n\nNew columns added:\n- Language features: 97\n- Category features: 112\n- Total features: 222\n"
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "application/vnd.hex.export+parquet": {
              "success": true,
              "exportKey": "019bae66-d350-7001-a251-d171be1cf243/019be885-c92a-7aac-b4fd-d91fd28ebe86/exports/019be8ab-1f87-7000-85d7-1504e519dd4d"
            },
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>page_id</th>\n      <th>title</th>\n      <th>url</th>\n      <th>content_length</th>\n      <th>num_links</th>\n      <th>num_sections</th>\n      <th>num_references</th>\n      <th>num_categories</th>\n      <th>summary_length</th>\n      <th>avg_sentence_length</th>\n      <th>...</th>\n      <th>cat_Sky and weather gods</th>\n      <th>cat_Use dmy dates from April 2022</th>\n      <th>cat_Use dmy dates from December 2022</th>\n      <th>cat_Use dmy dates from November 2022</th>\n      <th>cat_Vicinal diols</th>\n      <th>cat_Webarchive template wayback links</th>\n      <th>cat_Wikipedia articles incorporating a citation from the 1911 Encyclopaedia Britannica with Wikisource reference</th>\n      <th>cat_Wikipedia articles needing clarification from May 2019</th>\n      <th>cat_Wikipedia articles needing page number citations from December 2022</th>\n      <th>cat_Wikipedia articles needing page number citations from October 2022</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>47262</td>\n      <td>2 Pallas</td>\n      <td>https://en.wikipedia.org/wiki/2_Pallas</td>\n      <td>49731</td>\n      <td>184</td>\n      <td>16</td>\n      <td>138</td>\n      <td>20</td>\n      <td>2198</td>\n      <td>24.40</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>71318</td>\n      <td>100-year flood</td>\n      <td>https://en.wikipedia.org/wiki/100-year_flood</td>\n      <td>24708</td>\n      <td>265</td>\n      <td>10</td>\n      <td>43</td>\n      <td>9</td>\n      <td>698</td>\n      <td>25.20</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>175149</td>\n      <td>'Pataphysics</td>\n      <td>https://en.wikipedia.org/wiki/%27Pataphysics</td>\n      <td>49610</td>\n      <td>51</td>\n      <td>24</td>\n      <td>65</td>\n      <td>19</td>\n      <td>273</td>\n      <td>20.50</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>786020</td>\n      <td>1,4-Butanediol</td>\n      <td>https://en.wikipedia.org/wiki/1,4-Butanediol</td>\n      <td>19934</td>\n      <td>0</td>\n      <td>12</td>\n      <td>32</td>\n      <td>25</td>\n      <td>369</td>\n      <td>25.00</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1110853</td>\n      <td>*Dyēus</td>\n      <td>https://en.wikipedia.org/wiki/*Dy%C4%93us</td>\n      <td>62518</td>\n      <td>0</td>\n      <td>18</td>\n      <td>60</td>\n      <td>28</td>\n      <td>839</td>\n      <td>20.67</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 222 columns</p>\n</div>"
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    }
  ],
  "metadata": {
    "orig_nbformat": 4,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "hex_info": {
      "author": "Lucas Campagnaro",
      "project_id": "019be885-c92a-7aac-b4fd-d91fd28ebe86",
      "version": "draft",
      "exported_date": "Fri Jan 23 2026 02:51:51 GMT+0000 (Coordinated Universal Time)"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}