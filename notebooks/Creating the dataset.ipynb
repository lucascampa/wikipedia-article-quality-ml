{"cells":[{"cell_type":"code","source":"import pandas as _hex_pandas\nimport datetime as _hex_datetime\nimport json as _hex_json","execution_count":null,"metadata":{},"outputs":[]},{"cell_type":"code","source":"hex_scheduled = _hex_json.loads(\"false\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_user_email = _hex_json.loads(\"\\\"example-user@example.com\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_user_attributes = _hex_json.loads(\"{}\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_run_context = _hex_json.loads(\"\\\"logic\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_timezone = _hex_json.loads(\"\\\"UTC\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_project_id = _hex_json.loads(\"\\\"019bae67-af4d-7000-baed-c8d253b14659\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_project_name = _hex_json.loads(\"\\\"Creating the dataset\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_status = _hex_json.loads(\"\\\"\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_categories = _hex_json.loads(\"[]\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_color_palette = _hex_json.loads(\"[\\\"#4C78A8\\\",\\\"#F58518\\\",\\\"#E45756\\\",\\\"#72B7B2\\\",\\\"#54A24B\\\",\\\"#EECA3B\\\",\\\"#B279A2\\\",\\\"#FF9DA6\\\",\\\"#9D755D\\\",\\\"#BAB0AC\\\"]\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"markdown","source":"## Wikipedia Scientific Articles Dataset\n\n**Goal**: Extract and analyze Wikipedia articles about scientific subjects, filtering out non-scientific content (personalities, organizations, events).\n\n**Process**:\n\n1. **Extract article names & IDs** from Wikipedia list pages (glossary pages, science topic lists)\n   - Input: `lists.txt` containing Wikipedia list page titles\n   - Uses Wikipedia's query API with `generator=links` to get article titles + page IDs\n   - Handles pagination via continuation tokens (500 results/request)\n   - Deduplicates across lists â†’ outputs `article_names.csv`\n\n2. **Fetch article metadata** in batches\n   - Batch requests: 50 page IDs per API call (~473 batches for 23k articles)\n   - Uses async with concurrency limiting (respects Wikipedia rate limits)\n   - Fetches: content length, links, sections, references, categories (all/meta/science), languages, revision dates, summaries\n   - **Why page IDs**: Titles with special characters (commas in chemical names) break batch API; IDs are cleaner\n\n3. **Filter dataset** to remove non-scientific articles\n   - Currently identifying keywords for: people, organizations, events, letter/list articles\n   - Will use categories, content patterns, and metadata to classify articles\n\n**Current status**: ~22.6k articles fetched with full metadata, ready for filtering.","metadata":{}},{"cell_type":"markdown","source":"## Getting the articles names and IDs\n\nAPI calls are expensive, so to save time I first got only the articles names and IDs.\n\n","metadata":{}},{"cell_type":"markdown","source":"API calls are expensive, so to save time I first got only the articles names and IDs.\n\n","metadata":{}},{"cell_type":"code","source":"# This is for reference, Claude. Do not execute this cell. Just read it\n# Here is a code snippet that illustrates the other columns. Apply this context\n# On the next cell\n \n# Categories\nall_cats = []\nmeta_cats = []\nif 'categories' in page:\n    for cat in page['categories']:\n        cat_name = cat['title'].replace('Category:', '')\n        all_cats.append(cat_name)\n        if 'hidden' in cat:\n            meta_cats.append(cat_name)\n\n# Revisions\nnum_revs = len(page.get('revisions', []))\nlast_rev = page.get('revisions', [{}])[0].get('timestamp', None) if num_revs > 0 else None\n\n# Languages\nlangs = [ll['lang'] for ll in page.get('langlinks', [])]\n\nreturn {\n    'all_categories': all_cats,\n    'meta_categories': meta_cats,\n    'science_categories': [c for c in all_cats if c not in meta_cats],\n    'last_revision_date': last_rev,\n    'num_languages': len(langs),\n    'languages': langs,\n    'rating': None\n}\n        ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import requests\n\ndef extract_articles_from_list(page_title):\n    \"\"\"Extract article links with page IDs from a Wikipedia list page\"\"\"\n    try:\n        url = \"https://en.wikipedia.org/w/api.php\"\n        headers = {'User-Agent': 'WikipediaBot/1.0 (Educational Project)'}\n        \n        all_articles = []\n        continue_params = {}\n        \n        while True:\n            params = {\n                'action': 'query',\n                'generator': 'links',\n                'titles': page_title,\n                'gpllimit': 'max',\n                'gplnamespace': 0,\n                'format': 'json'\n            }\n            params.update(continue_params)\n            \n            response = requests.get(url, params=params, headers=headers, timeout=10)\n            data = response.json()\n            \n            if 'query' in data and 'pages' in data['query']:\n                articles = [(page.get('title'), page.get('pageid')) \n                           for page in data['query']['pages'].values()\n                           if page.get('pageid') is not None]\n                all_articles.extend(articles)\n            \n            # Check for continuation\n            if 'continue' in data:\n                continue_params = data['continue']\n            else:\n                break\n        \n        return all_articles\n        \n    except Exception as e:\n        print(f\"  Error extracting from {page_title}: {e}\")\n        return []","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import csv\n\narticles = []\n\nwith open('lists.txt', 'r', encoding='utf-8') as f:\n    list_pages = [line.strip() for line in f if line.strip()]\n\nfor page in list_pages:\n    articles.extend(extract_articles_from_list(page))\n\n# Remove duplicates (by title)\nseen_titles = set()\nunique_articles = []\nfor title, page_id in articles:\n    if title not in seen_titles:\n        seen_titles.add(title)\n        unique_articles.append((title, page_id))\n\nunique_articles.sort(key=lambda x: x[0])\n\n# Save to CSV\nwith open('article_names.csv', 'w', newline='', encoding='utf-8') as f:\n    writer = csv.writer(f)\n    writer.writerow(['title', 'page_id'])\n    writer.writerows(unique_articles)\n\nprint(f\"Saved {len(unique_articles)} unique articles to article_names.csv\")","metadata":{},"execution_count":null,"outputs":[{"data":{"text/plain":"Saved 22633 unique articles to article_names.csv\n"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","source":"## Creating the dataset\n\n","metadata":{}},{"cell_type":"code","source":"import aiohttp\nimport asyncio\n\nasync def fetch_article_data_batch(page_ids, session):\n    \"\"\"Fetch metadata for up to 50 articles in a single API call\"\"\"\n    url = \"https://en.wikipedia.org/w/api.php\"\n    headers = {'User-Agent': 'WikipediaBot/1.0 (Educational Project)'}\n    \n    # Join page IDs with pipe separator (Wikipedia batch API format)\n    ids_param = '|'.join(str(pid) for pid in page_ids)\n    \n    params = {\n        'action': 'query',\n        'pageids': ids_param,\n        'prop': 'info|revisions|links|categories|extracts|langlinks',\n        'inprop': 'url',\n        'rvprop': 'content|timestamp',\n        'rvslots': 'main',\n        'pllimit': 'max',\n        'cllimit': 'max',\n        'lllimit': 'max',\n        'exintro': 'true',\n        'explaintext': 'true',\n        'format': 'json'\n    }\n    \n    try:\n        async with session.get(url, params=params, headers=headers, timeout=30) as response:\n            data = await response.json()\n            \n            if 'query' not in data or 'pages' not in data['query']:\n                return []\n            \n            results = []\n            for page in data['query']['pages'].values():\n                # Skip if page doesn't exist\n                if 'missing' in page or 'invalid' in page:\n                    continue\n                \n                # Extract basic info\n                page_id = page.get('pageid', None)\n                title = page.get('title', '')\n                url = page.get('fullurl', '')\n                \n                # Get content\n                content = ''\n                if 'revisions' in page and len(page['revisions']) > 0:\n                    content = page['revisions'][0].get('slots', {}).get('main', {}).get('*', '')\n                \n                content_length = len(content)\n                num_sections = content.count('\\n==')\n                num_references = content.count('<ref')\n                num_links = len(page.get('links', []))\n                \n                # Categories\n                all_cats = []\n                meta_cats = []\n                if 'categories' in page:\n                    for cat in page['categories']:\n                        cat_name = cat['title'].replace('Category:', '')\n                        all_cats.append(cat_name)\n                        if 'hidden' in cat:\n                            meta_cats.append(cat_name)\n                \n                science_cats = [c for c in all_cats if c not in meta_cats]\n                \n                # Revisions\n                num_revs = len(page.get('revisions', []))\n                last_rev = page.get('revisions', [{}])[0].get('timestamp', None) if num_revs > 0 else None\n                \n                # Languages\n                langs = [ll['lang'] for ll in page.get('langlinks', [])]\n                \n                summary = page.get('extract', '')\n                summary_length = len(summary)\n                \n                sentences = [s.strip() for s in summary.split('.') if s.strip()]\n                avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences) if sentences else 0\n                \n                results.append({\n                    'page_id': page_id,\n                    'title': title,\n                    'url': url,\n                    'content_length': content_length,\n                    'num_links': num_links,\n                    'num_sections': num_sections,\n                    'num_references': num_references,\n                    'all_categories': all_cats,\n                    'meta_categories': meta_cats,\n                    'science_categories': science_cats,\n                    'num_categories': len(all_cats),\n                    'summary_length': summary_length,\n                    'avg_sentence_length': round(avg_sentence_length, 2),\n                    'last_revision_date': last_rev,\n                    'num_languages': len(langs),\n                    'languages': langs\n                })\n            \n            return results\n        \n    except Exception as e:\n        print(f\"Error fetching batch: {e}\")\n        return []\n\n\nasync def batch_fetch_metadata(page_ids, batch_size=50):\n    \"\"\"Fetch metadata for all articles using batched requests\"\"\"\n    from tqdm.auto import tqdm\n    \n    all_results = []\n    \n    # Split into batches\n    batches = [page_ids[i:i+batch_size] for i in range(0, len(page_ids), batch_size)]\n    \n    async with aiohttp.ClientSession() as session:\n        semaphore = asyncio.Semaphore(1)\n        \n        async def fetch_batch_with_limit(batch):\n            async with semaphore:\n                await asyncio.sleep(0.5)\n                return await fetch_article_data_batch(batch, session)\n        \n        tasks = [fetch_batch_with_limit(batch) for batch in batches]\n        \n        for coro in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc=\"Fetching batches\"):\n            batch_results = await coro\n            all_results.extend(batch_results)\n    \n    return all_results\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nimport pandas as pd\n\n# Load page IDs from CSV\ndf = pd.read_csv('article_names.csv')\npage_ids = df['page_id'].dropna().astype(int).tolist()\n\nprint(f\"Fetching metadata for {len(page_ids)} articles...\")\n\n# Run async code\nstart_time = time.perf_counter()\narticle_data = await batch_fetch_metadata(page_ids)\nend_time = time.perf_counter()\n\nprint(f\"\\nCompleted in {end_time - start_time:.1f} seconds\")\nprint(f\"Successfully fetched: {len(article_data)} articles\")\nprint(f\"Failed/missing: {len(page_ids) - len(article_data)} articles\")","metadata":{},"execution_count":null,"outputs":[{"data":{"text/plain":"Fetching metadata for 22633 articles...\n"},"execution_count":null,"metadata":{},"output_type":"execute_result"},{"data":{"text/plain":"Fetching batches:   0%|          | 0/453 [00:00<?, ?it/s]"},"execution_count":null,"metadata":{},"output_type":"execute_result"},{"data":{"text/plain":"\nCompleted in 478.8 seconds\nSuccessfully fetched: 22633 articles\nFailed/missing: 0 articles\n"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","source":"import aiohttp\nimport asyncio\nimport json\n\n# Test with first 3 page IDs\ntest_ids = page_ids[:3]\nprint(f\"Testing with page IDs: {test_ids}\")\n\nasync def test_api_response():\n    url = \"https://en.wikipedia.org/w/api.php\"\n    headers = {'User-Agent': 'WikipediaBot/1.0 (Educational Project)'}\n    ids_param = '|'.join(str(pid) for pid in test_ids)\n    \n    params = {\n        'action': 'query',\n        'pageids': ids_param,\n        'prop': 'info|revisions|links|categories|extracts|langlinks',\n        'inprop': 'url',\n        'rvprop': 'content|timestamp',\n        'rvslots': 'main',\n        'pllimit': 'max',\n        'cllimit': 'max',\n        'clshow': '!hidden|hidden',\n        'lllimit': 'max',\n        'exintro': 'true',\n        'explaintext': 'true',\n        'format': 'json'\n    }\n    \n    async with aiohttp.ClientSession() as session:\n        async with session.get(url, params=params, headers=headers, timeout=30) as response:\n            data = await response.json()\n            return data\n\nraw_response = await test_api_response()\nprint(f\"\\nResponse keys: {raw_response.keys()}\")\n\nif 'error' in raw_response:\n    print(f\"\\nERROR from Wikipedia API:\")\n    print(json.dumps(raw_response['error'], indent=2))\nelif 'query' in raw_response:\n    print(f\"Query keys: {raw_response['query'].keys()}\")\n    if 'pages' in raw_response['query']:\n        print(f\"Number of pages: {len(raw_response['query']['pages'])}\")\n        print(f\"\\nFirst page:\")\n        first_page = list(raw_response['query']['pages'].values())[0]\n        print(json.dumps(first_page, indent=2)[:1000])  # First 1000 chars","metadata":{},"execution_count":null,"outputs":[{"data":{"text/plain":"Testing with page IDs: [175149, 1215764, 37376135]\n\nResponse keys: dict_keys(['error', 'servedby'])\n\nERROR from Wikipedia API:\n{\n  \"code\": \"show\",\n  \"info\": \"Incorrect parameter - mutually exclusive values may not be supplied.\",\n  \"*\": \"See https://en.wikipedia.org/w/api.php for API usage. Subscribe to the mediawiki-api-announce mailing list at &lt;https://lists.wikimedia.org/postorius/lists/mediawiki-api-announce.lists.wikimedia.org/&gt; for notice of API deprecations and breaking changes.\"\n}\n"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","source":"df = pd.DataFrame(article_data)\ndf.to_csv('article_data.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"people_keywords = {'births', 'deaths', 'people', 'living'}\nletter_keywords = {'letters', 'letter', 'alphabet'}\nevent_keywords = {'events', 'battles', 'wars', 'conflicts', 'disasters', 'treaties'}\norg_keywords = {'organizations', 'organisations', 'companies', 'institutions', 'universities', 'agencies'}","metadata":{},"execution_count":null,"outputs":[]}],"metadata":{"orig_nbformat":4,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"hex_info":{"author":"Lucas Campagnaro","project_id":"019bae67-af4d-7000-baed-c8d253b14659","version":"draft","exported_date":"Tue Jan 13 2026 00:58:55 GMT+0000 (Coordinated Universal Time)"}},"nbformat":4,"nbformat_minor":4}