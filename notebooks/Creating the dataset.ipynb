{"cells":[{"cell_type":"code","source":"import pandas as _hex_pandas\nimport datetime as _hex_datetime\nimport json as _hex_json","execution_count":null,"metadata":{},"outputs":[]},{"cell_type":"code","source":"hex_scheduled = _hex_json.loads(\"false\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_user_email = _hex_json.loads(\"\\\"example-user@example.com\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_user_attributes = _hex_json.loads(\"{}\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_run_context = _hex_json.loads(\"\\\"logic\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_timezone = _hex_json.loads(\"\\\"UTC\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_project_id = _hex_json.loads(\"\\\"019bae67-af4d-7000-baed-c8d253b14659\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_project_name = _hex_json.loads(\"\\\"Creating the dataset\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_status = _hex_json.loads(\"\\\"\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_categories = _hex_json.loads(\"[]\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_color_palette = _hex_json.loads(\"[\\\"#4C78A8\\\",\\\"#F58518\\\",\\\"#E45756\\\",\\\"#72B7B2\\\",\\\"#54A24B\\\",\\\"#EECA3B\\\",\\\"#B279A2\\\",\\\"#FF9DA6\\\",\\\"#9D755D\\\",\\\"#BAB0AC\\\"]\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"markdown","source":"## Getting the articles names\n\nAPI calls are expensive, so to save time I first got only the articles names.\n\n","metadata":{}},{"cell_type":"code","source":"import requests\n\ndef extract_articles_from_list(page_title):\n    \"\"\"Try to extract article links from a Wikipedia list page\"\"\"\n    try:\n        url = \"https://en.wikipedia.org/w/api.php\"\n        headers = {'User-Agent': 'WikipediaBot/1.0 (Educational Project)'}\n        \n        params = {\n            'action': 'parse',\n            'page': page_title,\n            'prop': 'links',\n            'format': 'json'\n        }\n        \n        response = requests.get(url, params=params, headers=headers, timeout=10)\n        data = response.json()\n        \n        if 'parse' in data and 'links' in data['parse']:\n            # Get all internal links (namespace 0 = main articles)\n            articles = [link['*'] for link in data['parse']['links'] \n                       if link.get('ns') == 0]\n            return articles\n        \n        return []\n        \n    except Exception as e:\n        print(f\"  Error extracting from {page_title}: {e}\")\n        return []\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"articles = set()\n\n# Read list pages\nwith open('lists.txt', 'r', encoding='utf-8') as f:\n    list_pages = [line.strip() for line in f if line.strip()]\n\nfor page in list_pages:\n    articles.update(extract_articles_from_list(page))\n\narticles = sorted(articles)\n\nwith open('article_names.txt', 'w', encoding='utf-8') as f:\n    f.write('\\n'.join(articles))    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating the dataset\n\n","metadata":{}},{"cell_type":"markdown","source":"### Simplified version\n\n","metadata":{}},{"cell_type":"code","source":"import aiohttp\nimport asyncio\n\nasync def fetch_article_data(article_title, session):\n    \"\"\"Fetch straightforward article metadata\"\"\"\n    url = \"https://en.wikipedia.org/w/api.php\"\n    headers = {'User-Agent': 'WikipediaBot/1.0 (Educational Project)'}\n    \n    # Get page info, content, links, categories, references\n    params = {\n        'action': 'query',\n        'titles': article_title,\n        'prop': 'info|revisions|links|categories|extracts',\n        'inprop': 'url',\n        'rvprop': 'content|timestamp',\n        'rvslots': 'main',\n        'pllimit': 'max',\n        'cllimit': 'max',\n        'exintro': 'true',\n        'explaintext': 'true',\n        'format': 'json'\n    }\n    \n    try:\n        async with session.get(url, params=params, headers=headers, timeout=10) as response:\n            data = await response.json()\n            \n            if 'query' not in data or 'pages' not in data['query']:\n                return None\n            \n            page = list(data['query']['pages'].values())[0]\n            \n            # Skip if page doesn't exist\n            if 'missing' in page or 'invalid' in page:\n                return None\n            \n            # Extract basic info\n            title = page.get('title', '')\n            url = page.get('fullurl', '')\n            \n            # Get content\n            content = ''\n            if 'revisions' in page and len(page['revisions']) > 0:\n                content = page['revisions'][0].get('slots', {}).get('main', {}).get('*', '')\n            \n            content_length = len(content)\n            \n            # Count sections (## markers in wikitext)\n            num_sections = content.count('\\n==')\n            \n            # Count references\n            num_references = content.count('<ref')\n            \n            # Count links\n            num_links = len(page.get('links', []))\n            \n            # Count categories\n            num_categories = len(page.get('categories', []))\n            \n            # Get summary/extract\n            summary = page.get('extract', '')\n            summary_length = len(summary)\n            \n            # Calculate average sentence length (rough approximation)\n            sentences = [s.strip() for s in summary.split('.') if s.strip()]\n            avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences) if sentences else 0\n            \n            return {\n                'title': title,\n                'url': url,\n                'content_length': content_length,\n                'num_links': num_links,\n                'num_sections': num_sections,\n                'num_references': num_references,\n                'num_categories': num_categories,\n                'summary_length': summary_length,\n                'avg_sentence_length': round(avg_sentence_length, 2)\n            }\n        \n    except Exception as e:\n        print(f\"Error fetching {article_title}: {e}\")\n        return None\n\n\nasync def fetch_all_metadata(article_names):\n    \"\"\"Fetch metadata for all articles with concurrency control\"\"\"\n    from tqdm.auto import tqdm\n    \n    results = []\n    \n    async with aiohttp.ClientSession() as session:\n        # Limit to 15 concurrent requests\n        semaphore = asyncio.Semaphore(15)\n        \n        async def fetch_with_limit(article):\n            async with semaphore:\n                return await fetch_article_data(article, session)\n        \n        # Create tasks with progress bar\n        tasks = [fetch_with_limit(article) for article in article_names]\n        \n        # Run with progress tracking\n        for coro in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc=\"Fetching\"):\n            result = await coro\n            if result:\n                results.append(result)\n    \n    return results","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\n\n# Load article names\nwith open('article_names.txt', 'r', encoding='utf-8') as f:\n    article_names = [line.strip() for line in f if line.strip()]\n\nprint(f\"Fetching metadata for {len(article_names)} articles...\")\n\n# Run async code\nstart_time = time.perf_counter()\narticle_data = await fetch_all_metadata(article_names)\nend_time = time.perf_counter()\n\nprint(f\"\\nCompleted in {end_time - start_time:.1f} seconds\")\nprint(f\"Successfully fetched: {len(article_data)} articles\")\nprint(f\"Failed/missing: {len(article_names) - len(article_data)} articles\")","metadata":{},"execution_count":null,"outputs":[{"data":{"text/plain":"Fetching metadata for 23633 articles...\n"},"execution_count":null,"metadata":{},"output_type":"execute_result"},{"data":{"traceback":"\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)\n\u001b[0;32m/tmp/ipykernel_36/777206550.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Run async code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0marticle_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mfetch_all_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;31mTypeError\u001b[0m: fetch_all_metadata() takes 0 positional arguments but 1 was given"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]}],"metadata":{"orig_nbformat":4,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"hex_info":{"author":"Lucas Campagnaro","project_id":"019bae67-af4d-7000-baed-c8d253b14659","version":"draft","exported_date":"Mon Jan 12 2026 01:40:56 GMT+0000 (Coordinated Universal Time)"}},"nbformat":4,"nbformat_minor":4}