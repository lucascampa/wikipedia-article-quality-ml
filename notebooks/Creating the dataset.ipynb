{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as _hex_pandas\n",
        "import datetime as _hex_datetime\n",
        "import json as _hex_json"
      ],
      "execution_count": null,
      "metadata": {
        "id": "xuoCrfisxjGy"
      },
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hex_scheduled = _hex_json.loads(\"false\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "KNr39AUQxjG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hex_user_email = _hex_json.loads(\"\\\"example-user@example.com\\\"\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "DkW716DGxjG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hex_user_attributes = _hex_json.loads(\"{}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "lTJpLPJRxjG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hex_run_context = _hex_json.loads(\"\\\"logic\\\"\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "41nfXrl2xjG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hex_timezone = _hex_json.loads(\"\\\"UTC\\\"\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "3pbcV0aDxjG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hex_project_id = _hex_json.loads(\"\\\"019bae67-af4d-7000-baed-c8d253b14659\\\"\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "-Q0KofgQxjG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hex_project_name = _hex_json.loads(\"\\\"Creating the dataset\\\"\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "_egoyx_HxjG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hex_status = _hex_json.loads(\"\\\"\\\"\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "qehkJQDfxjG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hex_categories = _hex_json.loads(\"[]\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "kCgQm9EhxjG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hex_color_palette = _hex_json.loads(\"[\\\"#4C78A8\\\",\\\"#F58518\\\",\\\"#E45756\\\",\\\"#72B7B2\\\",\\\"#54A24B\\\",\\\"#EECA3B\\\",\\\"#B279A2\\\",\\\"#FF9DA6\\\",\\\"#9D755D\\\",\\\"#BAB0AC\\\"]\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "EKtKKIfvxjG8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are a lot of duplicates across the lists, so I'll have to extract just the names and IDs first\n",
        "\n"
      ],
      "metadata": {
        "id": "rqNUdj_cxjG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is for reference, Claude. Do not execute this cell. Just read it\n",
        "# Here is a code snippet that illustrates the other columns. Apply this context\n",
        "# On the next cell\n",
        "\n",
        "# Categories\n",
        "all_cats = []\n",
        "meta_cats = []\n",
        "if 'categories' in page:\n",
        "    for cat in page['categories']:\n",
        "        cat_name = cat['title'].replace('Category:', '')\n",
        "        all_cats.append(cat_name)\n",
        "        if 'hidden' in cat:\n",
        "            meta_cats.append(cat_name)\n",
        "\n",
        "# Revisions\n",
        "num_revs = len(page.get('revisions', []))\n",
        "last_rev = page.get('revisions', [{}])[0].get('timestamp', None) if num_revs > 0 else None\n",
        "\n",
        "# Languages\n",
        "langs = [ll['lang'] for ll in page.get('langlinks', [])]\n",
        "\n",
        "return {\n",
        "    'all_categories': all_cats,\n",
        "    'meta_categories': meta_cats,\n",
        "    'science_categories': [c for c in all_cats if c not in meta_cats],\n",
        "    'last_revision_date': last_rev,\n",
        "    'num_languages': len(langs),\n",
        "    'languages': langs,\n",
        "    'rating': None\n",
        "}\n",
        ""
      ],
      "metadata": {
        "id": "u4k4HtfKxjHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def extract_articles_from_list(page_title):\n",
        "    \"\"\"Extract article links with page IDs from a Wikipedia list page\"\"\"\n",
        "    try:\n",
        "        url = \"https://en.wikipedia.org/w/api.php\"\n",
        "        headers = {'User-Agent': 'WikipediaBot/1.0 (Educational Project)'}\n",
        "\n",
        "        all_articles = []\n",
        "        continue_params = {}\n",
        "\n",
        "        while True:\n",
        "            params = {\n",
        "                'action': 'query',\n",
        "                'generator': 'links',\n",
        "                'titles': page_title,\n",
        "                'gpllimit': 'max',\n",
        "                'gplnamespace': 0,\n",
        "                'format': 'json'\n",
        "            }\n",
        "            params.update(continue_params)\n",
        "\n",
        "            response = requests.get(url, params=params, headers=headers, timeout=10)\n",
        "            data = response.json()\n",
        "\n",
        "            if 'query' in data and 'pages' in data['query']:\n",
        "                articles = [(page.get('title'), page.get('pageid'))\n",
        "                           for page in data['query']['pages'].values()\n",
        "                           if page.get('pageid') is not None]\n",
        "                all_articles.extend(articles)\n",
        "\n",
        "            # Check for continuation\n",
        "            if 'continue' in data:\n",
        "                continue_params = data['continue']\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        return all_articles\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  Error extracting from {page_title}: {e}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "G3prNRtdxjHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "articles = []\n",
        "\n",
        "with open('lists.txt', 'r', encoding='utf-8') as f:\n",
        "    list_pages = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "for page in list_pages:\n",
        "    articles.extend(extract_articles_from_list(page))\n",
        "\n",
        "# Remove duplicates (by title)\n",
        "seen_titles = set()\n",
        "unique_articles = []\n",
        "for title, page_id in articles:\n",
        "    if title not in seen_titles:\n",
        "        seen_titles.add(title)\n",
        "        unique_articles.append((title, page_id))\n",
        "\n",
        "unique_articles.sort(key=lambda x: x[0])\n",
        "\n",
        "# Save to CSV\n",
        "with open('article_names.csv', 'w', newline='', encoding='utf-8') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(['title', 'page_id'])\n",
        "    writer.writerows(unique_articles)\n",
        "\n",
        "print(f\"Saved {len(unique_articles)} unique articles to article_names.csv\")"
      ],
      "metadata": {
        "id": "ZRn8_9YgxjHA",
        "outputId": "780e8023-f29b-402f-8a2f-71f536018ce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": "Saved 22633 unique articles to article_names.csv\n"
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "\n",
        "def validate_article_data(article):\n",
        "    \"\"\"Check if article has complete/valid data\"\"\"\n",
        "    # Must have categories (empty list = failed fetch)\n",
        "    if not article.get('all_categories') or len(article.get('all_categories', [])) == 0:\n",
        "        return False\n",
        "\n",
        "    # Should have some content\n",
        "    if article.get('content_length', 0) == 0:\n",
        "        return False\n",
        "\n",
        "    # Should have a title\n",
        "    if not article.get('title'):\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def fetch_article_data_batch(page_ids, retry_count=0):\n",
        "    \"\"\"Fetch metadata for up to 50 articles in a single API call - synchronous version\"\"\"\n",
        "    url = \"https://en.wikipedia.org/w/api.php\"\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
        "\n",
        "    # Join page IDs with pipe separator (Wikipedia batch API format)\n",
        "    ids_param = '|'.join(str(pid) for pid in page_ids)\n",
        "\n",
        "    params = {\n",
        "        'action': 'query',\n",
        "        'pageids': ids_param,\n",
        "        'prop': 'info|revisions|links|categories|extracts|langlinks',\n",
        "        'inprop': 'url',\n",
        "        'rvprop': 'content|timestamp',\n",
        "        'rvslots': 'main',\n",
        "        'pllimit': 'max',\n",
        "        'cllimit': 'max',\n",
        "        'clprop': 'hidden',\n",
        "        'lllimit': 'max',\n",
        "        'exintro': 'true',\n",
        "        'explaintext': 'true',\n",
        "        'format': 'json'\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, params=params, headers=headers, timeout=30)\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            if retry_count < 3:\n",
        "                print(f\"  Retry {retry_count + 1}/3 for batch (HTTP {response.status_code})\")\n",
        "                time.sleep(2 ** retry_count)  # Exponential backoff\n",
        "                return fetch_article_data_batch(page_ids, retry_count + 1)\n",
        "            else:\n",
        "                print(f\"  ERROR: HTTP {response.status_code} - batch failed after 3 retries\")\n",
        "                return []\n",
        "\n",
        "        data = response.json()\n",
        "\n",
        "        if 'query' not in data or 'pages' not in data['query']:\n",
        "            return []\n",
        "\n",
        "        # Rate limiting between main request and talk page request\n",
        "        time.sleep(0.5)\n",
        "\n",
        "        # Now fetch talk pages for ratings\n",
        "        titles = [page.get('title', '') for page in data['query']['pages'].values() if 'title' in page]\n",
        "        talk_titles = [f\"Talk:{title}\" for title in titles]\n",
        "        talk_params = {\n",
        "            'action': 'query',\n",
        "            'titles': '|'.join(talk_titles),\n",
        "            'prop': 'categories',\n",
        "            'cllimit': 'max',\n",
        "            'format': 'json'\n",
        "        }\n",
        "\n",
        "        talk_data_map = {}\n",
        "        talk_response = requests.get(url, params=talk_params, headers=headers, timeout=30)\n",
        "        if talk_response.status_code == 200:\n",
        "            talk_json = talk_response.json()\n",
        "            if 'query' in talk_json and 'pages' in talk_json['query']:\n",
        "                for page in talk_json['query']['pages'].values():\n",
        "                    if 'title' in page and page['title'].startswith('Talk:'):\n",
        "                        original_title = page['title'].replace('Talk:', '', 1)\n",
        "                        talk_data_map[original_title] = page\n",
        "\n",
        "        results = []\n",
        "        for page in data['query']['pages'].values():\n",
        "            # Skip if page doesn't exist\n",
        "            if 'missing' in page or 'invalid' in page:\n",
        "                continue\n",
        "\n",
        "            # Extract basic info\n",
        "            page_id = page.get('pageid', None)\n",
        "            title = page.get('title', '')\n",
        "            url = page.get('fullurl', '')\n",
        "\n",
        "            # Get content\n",
        "            content = ''\n",
        "            if 'revisions' in page and len(page['revisions']) > 0:\n",
        "                content = page['revisions'][0].get('slots', {}).get('main', {}).get('*', '')\n",
        "\n",
        "            content_length = len(content)\n",
        "            num_sections = content.count('\\n==')\n",
        "            num_references = content.count('<ref')\n",
        "            num_links = len(page.get('links', []))\n",
        "\n",
        "            # Categories\n",
        "            all_cats = []\n",
        "            meta_cats = []\n",
        "            if 'categories' in page:\n",
        "                for cat in page['categories']:\n",
        "                    cat_name = cat['title'].replace('Category:', '')\n",
        "                    all_cats.append(cat_name)\n",
        "                    if 'hidden' in cat:\n",
        "                        meta_cats.append(cat_name)\n",
        "\n",
        "            science_cats = [c for c in all_cats if c not in meta_cats]\n",
        "\n",
        "            # Revisions\n",
        "            num_revs = len(page.get('revisions', []))\n",
        "            last_rev = page.get('revisions', [{}])[0].get('timestamp', None) if num_revs > 0 else None\n",
        "\n",
        "            # Languages\n",
        "            langs = [ll['lang'] for ll in page.get('langlinks', [])]\n",
        "\n",
        "            summary = page.get('extract', '')\n",
        "            summary_length = len(summary)\n",
        "\n",
        "            sentences = [s.strip() for s in summary.split('.') if s.strip()]\n",
        "            avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences) if sentences else 0\n",
        "\n",
        "            # Extract rating from talk page\n",
        "            rating = None\n",
        "            if title in talk_data_map and 'categories' in talk_data_map[title]:\n",
        "                talk_cats = [cat['title'].replace('Category:', '') for cat in talk_data_map[title]['categories']]\n",
        "                for cat in talk_cats:\n",
        "                    cat_lower = cat.lower()\n",
        "                    if '-class' in cat_lower and 'articles' in cat_lower:\n",
        "                        for rating_class in ['fa-class', 'a-class', 'ga-class', 'b-class', 'c-class', 'start-class', 'stub-class']:\n",
        "                            if cat_lower.startswith(rating_class):\n",
        "                                rating = rating_class.replace('-class', '').upper()\n",
        "                                break\n",
        "                        if rating:\n",
        "                            break\n",
        "\n",
        "            article = {\n",
        "                'page_id': page_id,\n",
        "                'title': title,\n",
        "                'url': url,\n",
        "                'content_length': content_length,\n",
        "                'num_links': num_links,\n",
        "                'num_sections': num_sections,\n",
        "                'num_references': num_references,\n",
        "                'all_categories': all_cats,\n",
        "                'meta_categories': meta_cats,\n",
        "                'science_categories': science_cats,\n",
        "                'num_categories': len(all_cats),\n",
        "                'summary_length': summary_length,\n",
        "                'avg_sentence_length': round(avg_sentence_length, 2),\n",
        "                'last_revision_date': last_rev,\n",
        "                'num_languages': len(langs),\n",
        "                'languages': langs,\n",
        "                'rating': rating\n",
        "            }\n",
        "\n",
        "            # Validate article data\n",
        "            if validate_article_data(article):\n",
        "                results.append(article)\n",
        "            else:\n",
        "                # Mark for retry\n",
        "                results.append(None)\n",
        "\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        if retry_count < 3:\n",
        "            print(f\"  Retry {retry_count + 1}/3 for batch (Exception: {e})\")\n",
        "            time.sleep(2 ** retry_count)\n",
        "            return fetch_article_data_batch(page_ids, retry_count + 1)\n",
        "        else:\n",
        "            print(f\"  ERROR: {e} - batch failed after 3 retries\")\n",
        "            return []\n",
        "\n",
        "\n",
        "def batch_fetch_metadata(page_ids, batch_size=50):\n",
        "    \"\"\"Fetch metadata for all articles using batched synchronous requests\"\"\"\n",
        "    from tqdm.auto import tqdm\n",
        "\n",
        "    all_results = []\n",
        "    failed_page_ids = []\n",
        "\n",
        "    # Split into batches\n",
        "    batches = [page_ids[i:i+batch_size] for i in range(0, len(page_ids), batch_size)]\n",
        "\n",
        "    for batch in tqdm(batches, desc=\"Fetching batches\"):\n",
        "        batch_results = fetch_article_data_batch(batch)\n",
        "\n",
        "        # Separate valid and invalid results\n",
        "        for i, result in enumerate(batch_results):\n",
        "            if result is None:\n",
        "                # This article failed validation - retry individually\n",
        "                failed_page_ids.append(batch[i])\n",
        "            else:\n",
        "                all_results.append(result)\n",
        "\n",
        "        # Rate limiting between batches\n",
        "        time.sleep(1.0)\n",
        "\n",
        "    # Retry failed articles individually\n",
        "    if failed_page_ids:\n",
        "        print(f\"\\n\\nRetrying {len(failed_page_ids)} failed articles individually...\")\n",
        "        for page_id in tqdm(failed_page_ids, desc=\"Retrying failed articles\"):\n",
        "            retry_results = fetch_article_data_batch([page_id])\n",
        "            if retry_results and retry_results[0] is not None:\n",
        "                all_results.append(retry_results[0])\n",
        "            else:\n",
        "                print(f\"  PERMANENTLY FAILED: page_id {page_id}\")\n",
        "            time.sleep(0.5)\n",
        "\n",
        "    return all_results\n"
      ],
      "metadata": {
        "id": "vUT_UT7CxjHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "# Load page IDs from CSV\n",
        "df = pd.read_csv('article_names.csv')\n",
        "page_ids = df['page_id'].dropna().astype(int).tolist()\n",
        "\n",
        "print(f\"Fetching metadata for {len(page_ids)} articles...\")\n",
        "\n",
        "# Run synchronous code\n",
        "start_time = time.perf_counter()\n",
        "article_data = batch_fetch_metadata(page_ids)\n",
        "end_time = time.perf_counter()\n",
        "\n",
        "print(f\"\\nCompleted in {end_time - start_time:.1f} seconds\")\n",
        "print(f\"Successfully fetched: {len(article_data)} articles\")\n",
        "print(f\"Failed/missing: {len(page_ids) - len(article_data)} articles\")\n"
      ],
      "metadata": {
        "id": "kZaW8SWqxjHD",
        "outputId": "9be8c7e1-939d-4379-e1e6-6183f90f98dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": "Fetching metadata for 22633 articles...\n"
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "text/plain": "Fetching batches:   0%|          | 0/453 [00:00<?, ?it/s]"
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "traceback": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n\u001b[0;32m/tmp/ipykernel_38/913799279.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0m_hex_pks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_execution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstract_dataframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaterialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_hex_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaterializeDataFramesArgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_hex_json\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{\\\"read_names\\\":[\\\"batch_fetch_metadata\\\"],\\\"written_names\\\":[\\\"time\\\",\\\"pd\\\",\\\"df\\\",\\\"page_ids\\\",\\\"start_time\\\",\\\"article_data\\\",\\\"end_time\\\"]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapp_session_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_hex_APP_SESSION_TOKEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpython_kernel_init_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_hex_python_kernel_init_status\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhex_timezone\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_hex_kernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_or_none\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hex_timezone\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_hex_kernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterrupt_event\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_hex_interrupt_event\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0m_hex_pks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_execution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode_cell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'019baee4-29f2-7001-8a1c-77b043265725'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_code_b64\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'aW1wb3J0IHRpbWUKaW1wb3J0IHBhbmRhcyBhcyBwZAoKIyBMb2FkIHBhZ2UgSURzIGZyb20gQ1NWCmRmID0gcGQucmVhZF9jc3YoJ2FydGljbGVfbmFtZXMuY3N2JykKcGFnZV9pZHMgPSBkZlsncGFnZV9pZCddLmRyb3BuYSgpLmFzdHlwZShpbnQpLnRvbGlzdCgpCgpwcmludChmIkZldGNoaW5nIG1ldGFkYXRhIGZvciB7bGVuKHBhZ2VfaWRzKX0gYXJ0aWNsZXMuLi4iKQoKIyBSdW4gc3luY2hyb25vdXMgY29kZQpzdGFydF90aW1lID0gdGltZS5wZXJmX2NvdW50ZXIoKQphcnRpY2xlX2RhdGEgPSBiYXRjaF9mZXRjaF9tZXRhZGF0YShwYWdlX2lkcykKZW5kX3RpbWUgPSB0aW1lLnBlcmZfY291bnRlcigpCgpwcmludChmIlxuQ29tcGxldGVkIGluIHtlbmRfdGltZSAtIHN0YXJ0X3RpbWU6LjFmfSBzZWNvbmRzIikKcHJpbnQoZiJTdWNjZXNzZnVsbHkgZmV0Y2hlZDoge2xlbihhcnRpY2xlX2RhdGEpfSBhcnRpY2xlcyIpCnByaW50KGYiRmFpbGVkL21pc3Npbmc6IHtsZW4ocGFnZV9pZHMpIC0gbGVuKGFydGljbGVfZGF0YSl9IGFydGljbGVzIikK'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_globals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_locals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\n\u001b[0;32m/python-kernel-startup/python_kernel_startup/kernel_execution/code_cell.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(cell_id, user_code_b64, user_locals, user_globals)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0minstrument_ctx_with_metric_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"USER_CODE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_code_compiled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_globals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_locals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_locals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_var_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0muser_globals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_locals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32mhex_cell_019baee4-29f2-7001-8a1c-77b043265725.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Run synchronous code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0marticle_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_fetch_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32mhex_cell_019baed0-f0d3-7cca-b2e7-1ed23f4e5203.py\u001b[0m in \u001b[0;36mbatch_fetch_metadata\u001b[0;34m(page_ids, batch_size)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Fetching batches\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mbatch_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_article_data_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# Separate valid and invalid results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32mhex_cell_019baed0-f0d3-7cca-b2e7-1ed23f4e5203.py\u001b[0m in \u001b[0;36mfetch_article_data_batch\u001b[0;34m(page_ids, retry_count)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m~/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.11/lib/python3.11/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"get\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m~/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.11/lib/python3.11/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m~/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.11/lib/python3.11/site-packages/ddtrace/appsec/_common_module_patches.py\u001b[0m in \u001b[0;36mwrapped_request_D8CB81E472AF98A2\u001b[0;34m(original_request_callable, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0m_report_rasp_skipped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEXPLOIT_PREVENTION\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTYPE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSRF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0moriginal_request_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m~/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.11/lib/python3.11/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m~/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.11/lib/python3.11/site-packages/ddtrace/contrib/internal/requests/connection.py\u001b[0m in \u001b[0;36m_wrap_send\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse_headers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m~/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.11/lib/python3.11/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m~/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.11/lib/python3.11/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m~/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.11/lib/python3.11/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m             httplib_response = self._make_request(\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m~/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.11/lib/python3.11/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    443\u001b[0m                     \u001b[0;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m                     \u001b[0;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m~/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.11/lib/python3.11/site-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n\n\u001b[0;32m~/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.11/lib/python3.11/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1393\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1395\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1396\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/usr/lib/python3.11/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1312\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1314\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "article_data = pd.DataFrame(article_data)\n",
        "article_data.to_csv('article_data.csv', index=False)"
      ],
      "metadata": {
        "id": "tKis7-VBxjHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "people_keywords = {'births', 'deaths', 'people', 'living'}\n",
        "letter_keywords = {'letters', 'letter', 'alphabet'}\n",
        "event_keywords = {'events', 'battles', 'wars', 'conflicts', 'disasters', 'treaties', 'history'}\n",
        "org_keywords = {'organizations', 'organisations', 'companies', 'institutions', 'universities', 'agencies'}\n",
        "other_keywords = { 'book'}"
      ],
      "metadata": {
        "id": "qchjecQSxjHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved article data to examine structure\n",
        "import pandas as pd\n",
        "df_check = pd.read_csv('article_data.csv')\n",
        "print(f\"Shape: {df_check.shape}\")\n",
        "print(f\"\\nColumns: {df_check.columns.tolist()}\")\n",
        "print(f\"\\nSample of categories column:\")\n",
        "print(df_check['all_categories'].head(2))\n"
      ],
      "metadata": {
        "id": "p9h1LH9txjHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "\n",
        "def should_filter_article(categories_str, filter_keywords):\n",
        "    \"\"\"\n",
        "    Check if any category contains any of the filter keywords.\n",
        "    Returns True if article should be REMOVED.\n",
        "    \"\"\"\n",
        "    if pd.isna(categories_str) or categories_str == '[]':\n",
        "        return False\n",
        "\n",
        "    # Parse the string representation of list into actual list\n",
        "    try:\n",
        "        categories = ast.literal_eval(categories_str)\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "    # Check each category against all filter keywords\n",
        "    for category in categories:\n",
        "        category_lower = category.lower()\n",
        "        for keyword in filter_keywords:\n",
        "            if keyword in category_lower:\n",
        "                return True\n",
        "\n",
        "    return False\n",
        "\n",
        "# Combine all filter keywords\n",
        "all_filter_keywords = (\n",
        "    people_keywords |\n",
        "    letter_keywords |\n",
        "    event_keywords |\n",
        "    org_keywords |\n",
        "    other_keywords\n",
        ")\n",
        "\n",
        "print(f\"Total filter keywords: {len(all_filter_keywords)}\")\n",
        "print(f\"Keywords: {sorted(all_filter_keywords)}\")\n"
      ],
      "metadata": {
        "id": "C8SYIRcYxjHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "df = pd.read_csv('article_data.csv')\n",
        "\n",
        "print(f\"Original dataset: {len(df)} articles\")\n",
        "\n",
        "# Apply filter - mark articles to remove\n",
        "df['should_filter'] = df['all_categories'].apply(\n",
        "    lambda x: should_filter_article(x, all_filter_keywords)\n",
        ")\n",
        "\n",
        "# Show filtering stats\n",
        "filtered_out = df['should_filter'].sum()\n",
        "print(f\"Articles to filter out: {filtered_out} ({filtered_out/len(df)*100:.1f}%)\")\n",
        "print(f\"Articles remaining: {len(df) - filtered_out}\")\n",
        "\n",
        "# Create clean dataset\n",
        "df_clean = df[~df['should_filter']].drop(columns=['should_filter'])\n",
        "\n",
        "# Save filtered dataset\n",
        "df_clean.to_csv('article_data_filtered.csv', index=False)\n",
        "print(f\"\\nSaved filtered dataset to 'article_data_filtered.csv'\")\n"
      ],
      "metadata": {
        "id": "vdzD3ZLoxjHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the category-filtered dataset\n",
        "df_filtered = pd.read_csv('article_data_filtered.csv')\n",
        "\n",
        "print(f\"After category filtering: {len(df_filtered):,} articles\")\n",
        "\n",
        "# Check if rating column exists\n",
        "if 'rating' not in df_filtered.columns:\n",
        "    print(\"\\n  ERROR: 'rating' column not found in dataset\")\n",
        "    print(\"The metadata fetch may not have completed successfully.\")\n",
        "else:\n",
        "    # Filter to keep only articles WITH ratings\n",
        "    df_with_ratings = df_filtered[df_filtered['rating'].notna()].copy()\n",
        "\n",
        "    print(f\"\\nArticles WITH ratings: {len(df_with_ratings):,} ({len(df_with_ratings)/len(df_filtered)*100:.1f}%)\")\n",
        "    print(f\"Articles WITHOUT ratings: {(~df_filtered['rating'].notna()).sum():,}\")\n",
        "\n",
        "    # Show rating distribution\n",
        "    print(f\"\\nRating distribution:\")\n",
        "    rating_counts = df_with_ratings['rating'].value_counts().sort_index()\n",
        "    for rating, count in rating_counts.items():\n",
        "        print(f\"  {rating}: {count:,}\")\n",
        "\n",
        "    # Save final dataset\n",
        "    df_with_ratings.to_csv('article_data_final.csv', index=False)\n",
        "    print(f\"\\n Saved final dataset: {len(df_with_ratings):,} articles  'article_data_final.csv'\")\n"
      ],
      "metadata": {
        "id": "Cp8qIehxxjHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the old ML-ready dataset\n",
        "df_old = pd.read_csv('wikipedia_dataset_ml_ready.csv')\n",
        "\n",
        "# Load our current final dataset\n",
        "df_new = pd.read_csv('article_data_final.csv')\n",
        "\n",
        "print(\"OLD DATASET (wikipedia_dataset_ml_ready.csv):\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Articles: {len(df_old):,}\")\n",
        "print(f\"Columns: {df_old.columns.tolist()}\")\n",
        "print(f\"\\nShape: {df_old.shape}\")\n",
        "\n",
        "print(\"\\n\\nNEW DATASET (article_data_final.csv):\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Articles: {len(df_new):,}\")\n",
        "print(f\"Columns: {df_new.columns.tolist()}\")\n",
        "print(f\"\\nShape: {df_new.shape}\")\n",
        "\n",
        "print(\"\\n\\nDIFFERENCE:\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Article count difference: {len(df_old) - len(df_new):,}\")\n",
        "\n",
        "# Check rating distribution in old dataset\n",
        "if 'rating' in df_old.columns:\n",
        "    print(f\"\\nOld dataset rating distribution:\")\n",
        "    print(df_old['rating'].value_counts().sort_index())\n",
        "\n",
        "if 'rating' in df_new.columns:\n",
        "    print(f\"\\nNew dataset rating distribution:\")\n",
        "    print(df_new['rating'].value_counts().sort_index())\n"
      ],
      "metadata": {
        "id": "bdHvWAAtxjHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "\n",
        "# Load the filtered dataset\n",
        "df_final = pd.read_csv('article_data_final.csv')\n",
        "\n",
        "# Check for articles with 'agencies' in their categories\n",
        "articles_with_agencies = []\n",
        "\n",
        "for idx, row in df_final.iterrows():\n",
        "    try:\n",
        "        categories = ast.literal_eval(row['all_categories'])\n",
        "        for cat in categories:\n",
        "            if 'agencies' in cat.lower():\n",
        "                articles_with_agencies.append({\n",
        "                    'title': row['title'],\n",
        "                    'category': cat\n",
        "                })\n",
        "                break\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "print(f\"Articles with 'agencies' in categories: {len(articles_with_agencies)}\")\n",
        "print(\"\\nExamples:\")\n",
        "for item in articles_with_agencies[:10]:\n",
        "    print(f\"  - {item['title']}\")\n",
        "    print(f\"    Category: {item['category']}\")\n"
      ],
      "metadata": {
        "id": "44im-SjvxjHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "\n",
        "# Load the final dataset\n",
        "df_final = pd.read_csv('article_data_final.csv')\n",
        "\n",
        "# Find ANSMET article\n",
        "ansmet = df_final[df_final['page_id'] == 1851775]\n",
        "\n",
        "if len(ansmet) > 0:\n",
        "    print(f\"FOUND: {ansmet.iloc[0]['title']}\")\n",
        "    print(f\"Page ID: {ansmet.iloc[0]['page_id']}\")\n",
        "    print(f\"\\nAll categories:\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    try:\n",
        "        categories = ast.literal_eval(ansmet.iloc[0]['all_categories'])\n",
        "        for i, cat in enumerate(categories, 1):\n",
        "            print(f\"{i}. {cat}\")\n",
        "            # Check if it contains 'agencies'\n",
        "            if 'agencies' in cat.lower():\n",
        "                print(f\"   ^^^ CONTAINS 'agencies'!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing categories: {e}\")\n",
        "else:\n",
        "    print(\"Article with page_id 1851775 NOT FOUND in article_data_final.csv\")\n",
        "\n",
        "    # Check in the pre-rating-filter dataset\n",
        "    df_filtered = pd.read_csv('article_data_filtered.csv')\n",
        "    ansmet_filtered = df_filtered[df_filtered['page_id'] == 1851775]\n",
        "\n",
        "    if len(ansmet_filtered) > 0:\n",
        "        print(\"\\nBut it WAS found in article_data_filtered.csv (before rating filter)\")\n",
        "        print(\"This means it was filtered out due to missing rating, not categories\")\n",
        "    else:\n",
        "        print(\"\\nAlso NOT found in article_data_filtered.csv\")\n",
        "        print(\"Checking original article_data.csv...\")\n",
        "\n",
        "        df_original = pd.read_csv('article_data.csv')\n",
        "        ansmet_original = df_original[df_original['page_id'] == 1851775]\n",
        "\n",
        "        if len(ansmet_original) > 0:\n",
        "            print(\"\\nFOUND in article_data.csv (before category filtering)!\")\n",
        "            print(f\"Title: {ansmet_original.iloc[0]['title']}\")\n",
        "            print(f\"\\nCategories:\")\n",
        "            try:\n",
        "                categories = ast.literal_eval(ansmet_original.iloc[0]['all_categories'])\n",
        "                for i, cat in enumerate(categories, 1):\n",
        "                    print(f\"{i}. {cat}\")\n",
        "                    if 'agencies' in cat.lower():\n",
        "                        print(f\"   ^^^ CONTAINS 'agencies' - SHOULD HAVE BEEN FILTERED!\")\n",
        "            except:\n",
        "                pass\n"
      ],
      "metadata": {
        "id": "ycN-ittIxjHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look for should_remove_article function definition\n",
        "for i, cell in enumerate(notebook['cells']):\n",
        "    if cell['cell_type'] == 'code':\n",
        "        source = ''.join(cell['source'])\n",
        "        if 'should_remove_article' in source and 'def ' in source:\n",
        "            print(f\"Cell {i} - should_remove_article definition:\")\n",
        "            print(\"=\" * 70)\n",
        "            print(source)\n",
        "            print(\"\\n\" + \"=\" * 70)\n",
        "            break\n"
      ],
      "metadata": {
        "id": "TU47ExTbxjHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recreate old word-based matching\n",
        "def should_remove_OLD(categories_str, keywords):\n",
        "    \"\"\"Old approach: word-based matching\"\"\"\n",
        "    if pd.isna(categories_str) or categories_str == '[]':\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        categories = ast.literal_eval(categories_str)\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "    for cat in categories:\n",
        "        # Split category into words\n",
        "        words = {word.lower() for word in cat.split()}\n",
        "        # Check for intersection with keywords\n",
        "        if words & keywords:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# Test both approaches\n",
        "df_test = pd.read_csv('article_data.csv')\n",
        "\n",
        "# Apply OLD word-based filter\n",
        "old_filter = df_test['all_categories'].apply(\n",
        "    lambda x: should_remove_OLD(x, all_filter_keywords)\n",
        ")\n",
        "\n",
        "# Apply NEW substring filter (already done in C19, but let's recalculate)\n",
        "new_filter = df_test['all_categories'].apply(\n",
        "    lambda x: should_filter_article(x, all_filter_keywords)\n",
        ")\n",
        "\n",
        "print(\"Comparison:\")\n",
        "print(f\"Old word-based:    {old_filter.sum()} filtered out, {len(df_test) - old_filter.sum()} kept\")\n",
        "print(f\"New substring:     {new_filter.sum()} filtered out, {len(df_test) - new_filter.sum()} kept\")\n",
        "print(f\"\\nDifference: {abs(old_filter.sum() - new_filter.sum())} articles\")\n"
      ],
      "metadata": {
        "id": "uMiG-pBBxjHF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "orig_nbformat": 4,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "hex_info": {
      "author": "Lucas Campagnaro",
      "project_id": "019bae67-af4d-7000-baed-c8d253b14659",
      "version": "draft",
      "exported_date": "Fri Jan 23 2026 02:51:58 GMT+0000 (Coordinated Universal Time)"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}