{"cells":[{"cell_type":"code","source":"import pandas as _hex_pandas\nimport datetime as _hex_datetime\nimport json as _hex_json","execution_count":null,"metadata":{},"outputs":[]},{"cell_type":"code","source":"hex_scheduled = _hex_json.loads(\"false\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_user_email = _hex_json.loads(\"\\\"example-user@example.com\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_user_attributes = _hex_json.loads(\"{}\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_run_context = _hex_json.loads(\"\\\"logic\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_timezone = _hex_json.loads(\"\\\"UTC\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_project_id = _hex_json.loads(\"\\\"019bae67-af4d-7000-baed-c8d253b14659\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_project_name = _hex_json.loads(\"\\\"Creating the dataset\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_status = _hex_json.loads(\"\\\"\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_categories = _hex_json.loads(\"[]\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_color_palette = _hex_json.loads(\"[\\\"#4C78A8\\\",\\\"#F58518\\\",\\\"#E45756\\\",\\\"#72B7B2\\\",\\\"#54A24B\\\",\\\"#EECA3B\\\",\\\"#B279A2\\\",\\\"#FF9DA6\\\",\\\"#9D755D\\\",\\\"#BAB0AC\\\"]\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"markdown","source":"There are a lot of duplicates across the lists, so I'll have to extract just the names and IDs first\n\n","metadata":{}},{"cell_type":"code","source":"# This is for reference, Claude. Do not execute this cell. Just read it\n# Here is a code snippet that illustrates the other columns. Apply this context\n# On the next cell\n \n# Categories\nall_cats = []\nmeta_cats = []\nif 'categories' in page:\n    for cat in page['categories']:\n        cat_name = cat['title'].replace('Category:', '')\n        all_cats.append(cat_name)\n        if 'hidden' in cat:\n            meta_cats.append(cat_name)\n\n# Revisions\nnum_revs = len(page.get('revisions', []))\nlast_rev = page.get('revisions', [{}])[0].get('timestamp', None) if num_revs > 0 else None\n\n# Languages\nlangs = [ll['lang'] for ll in page.get('langlinks', [])]\n\nreturn {\n    'all_categories': all_cats,\n    'meta_categories': meta_cats,\n    'science_categories': [c for c in all_cats if c not in meta_cats],\n    'last_revision_date': last_rev,\n    'num_languages': len(langs),\n    'languages': langs,\n    'rating': None\n}\n        ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import requests\n\ndef extract_articles_from_list(page_title):\n    \"\"\"Extract article links with page IDs from a Wikipedia list page\"\"\"\n    try:\n        url = \"https://en.wikipedia.org/w/api.php\"\n        headers = {'User-Agent': 'WikipediaBot/1.0 (Educational Project)'}\n        \n        all_articles = []\n        continue_params = {}\n        \n        while True:\n            params = {\n                'action': 'query',\n                'generator': 'links',\n                'titles': page_title,\n                'gpllimit': 'max',\n                'gplnamespace': 0,\n                'format': 'json'\n            }\n            params.update(continue_params)\n            \n            response = requests.get(url, params=params, headers=headers, timeout=10)\n            data = response.json()\n            \n            if 'query' in data and 'pages' in data['query']:\n                articles = [(page.get('title'), page.get('pageid')) \n                           for page in data['query']['pages'].values()\n                           if page.get('pageid') is not None]\n                all_articles.extend(articles)\n            \n            # Check for continuation\n            if 'continue' in data:\n                continue_params = data['continue']\n            else:\n                break\n        \n        return all_articles\n        \n    except Exception as e:\n        print(f\"  Error extracting from {page_title}: {e}\")\n        return []","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import csv\n\narticles = []\n\nwith open('lists.txt', 'r', encoding='utf-8') as f:\n    list_pages = [line.strip() for line in f if line.strip()]\n\nfor page in list_pages:\n    articles.extend(extract_articles_from_list(page))\n\n# Remove duplicates (by title)\nseen_titles = set()\nunique_articles = []\nfor title, page_id in articles:\n    if title not in seen_titles:\n        seen_titles.add(title)\n        unique_articles.append((title, page_id))\n\nunique_articles.sort(key=lambda x: x[0])\n\n# Save to CSV\nwith open('article_names.csv', 'w', newline='', encoding='utf-8') as f:\n    writer = csv.writer(f)\n    writer.writerow(['title', 'page_id'])\n    writer.writerows(unique_articles)\n\nprint(f\"Saved {len(unique_articles)} unique articles to article_names.csv\")","metadata":{},"execution_count":null,"outputs":[{"data":{"text/plain":"Saved 22633 unique articles to article_names.csv\n"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","source":"import requests\nimport time\n\ndef validate_article_data(article):\n    \"\"\"Check if article has complete/valid data\"\"\"\n    # Must have categories (empty list = failed fetch)\n    if not article.get('all_categories') or len(article.get('all_categories', [])) == 0:\n        return False\n    \n    # Should have some content\n    if article.get('content_length', 0) == 0:\n        return False\n    \n    # Should have a title\n    if not article.get('title'):\n        return False\n    \n    return True\n\n\ndef fetch_article_data_batch(page_ids, retry_count=0):\n    \"\"\"Fetch metadata for up to 50 articles in a single API call - synchronous version\"\"\"\n    url = \"https://en.wikipedia.org/w/api.php\"\n    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n    \n    # Join page IDs with pipe separator (Wikipedia batch API format)\n    ids_param = '|'.join(str(pid) for pid in page_ids)\n    \n    params = {\n        'action': 'query',\n        'pageids': ids_param,\n        'prop': 'info|revisions|links|categories|extracts|langlinks',\n        'inprop': 'url',\n        'rvprop': 'content|timestamp',\n        'rvslots': 'main',\n        'pllimit': 'max',\n        'cllimit': 'max',\n        'clprop': 'hidden',\n        'lllimit': 'max',\n        'exintro': 'true',\n        'explaintext': 'true',\n        'format': 'json'\n    }\n    \n    try:\n        response = requests.get(url, params=params, headers=headers, timeout=30)\n        \n        if response.status_code != 200:\n            if retry_count < 3:\n                print(f\"  Retry {retry_count + 1}/3 for batch (HTTP {response.status_code})\")\n                time.sleep(2 ** retry_count)  # Exponential backoff\n                return fetch_article_data_batch(page_ids, retry_count + 1)\n            else:\n                print(f\"  ERROR: HTTP {response.status_code} - batch failed after 3 retries\")\n                return []\n        \n        data = response.json()\n        \n        if 'query' not in data or 'pages' not in data['query']:\n            return []\n        \n        # Rate limiting between main request and talk page request\n        time.sleep(0.5)\n        \n        # Now fetch talk pages for ratings\n        titles = [page.get('title', '') for page in data['query']['pages'].values() if 'title' in page]\n        talk_titles = [f\"Talk:{title}\" for title in titles]\n        talk_params = {\n            'action': 'query',\n            'titles': '|'.join(talk_titles),\n            'prop': 'categories',\n            'cllimit': 'max',\n            'format': 'json'\n        }\n        \n        talk_data_map = {}\n        talk_response = requests.get(url, params=talk_params, headers=headers, timeout=30)\n        if talk_response.status_code == 200:\n            talk_json = talk_response.json()\n            if 'query' in talk_json and 'pages' in talk_json['query']:\n                for page in talk_json['query']['pages'].values():\n                    if 'title' in page and page['title'].startswith('Talk:'):\n                        original_title = page['title'].replace('Talk:', '', 1)\n                        talk_data_map[original_title] = page\n        \n        results = []\n        for page in data['query']['pages'].values():\n            # Skip if page doesn't exist\n            if 'missing' in page or 'invalid' in page:\n                continue\n            \n            # Extract basic info\n            page_id = page.get('pageid', None)\n            title = page.get('title', '')\n            url = page.get('fullurl', '')\n            \n            # Get content\n            content = ''\n            if 'revisions' in page and len(page['revisions']) > 0:\n                content = page['revisions'][0].get('slots', {}).get('main', {}).get('*', '')\n            \n            content_length = len(content)\n            num_sections = content.count('\\n==')\n            num_references = content.count('<ref')\n            num_links = len(page.get('links', []))\n            \n            # Categories\n            all_cats = []\n            meta_cats = []\n            if 'categories' in page:\n                for cat in page['categories']:\n                    cat_name = cat['title'].replace('Category:', '')\n                    all_cats.append(cat_name)\n                    if 'hidden' in cat:\n                        meta_cats.append(cat_name)\n            \n            science_cats = [c for c in all_cats if c not in meta_cats]\n            \n            # Revisions\n            num_revs = len(page.get('revisions', []))\n            last_rev = page.get('revisions', [{}])[0].get('timestamp', None) if num_revs > 0 else None\n            \n            # Languages\n            langs = [ll['lang'] for ll in page.get('langlinks', [])]\n            \n            summary = page.get('extract', '')\n            summary_length = len(summary)\n            \n            sentences = [s.strip() for s in summary.split('.') if s.strip()]\n            avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences) if sentences else 0\n            \n            # Extract rating from talk page\n            rating = None\n            if title in talk_data_map and 'categories' in talk_data_map[title]:\n                talk_cats = [cat['title'].replace('Category:', '') for cat in talk_data_map[title]['categories']]\n                for cat in talk_cats:\n                    cat_lower = cat.lower()\n                    if '-class' in cat_lower and 'articles' in cat_lower:\n                        for rating_class in ['fa-class', 'a-class', 'ga-class', 'b-class', 'c-class', 'start-class', 'stub-class']:\n                            if cat_lower.startswith(rating_class):\n                                rating = rating_class.replace('-class', '').upper()\n                                break\n                        if rating:\n                            break\n            \n            article = {\n                'page_id': page_id,\n                'title': title,\n                'url': url,\n                'content_length': content_length,\n                'num_links': num_links,\n                'num_sections': num_sections,\n                'num_references': num_references,\n                'all_categories': all_cats,\n                'meta_categories': meta_cats,\n                'science_categories': science_cats,\n                'num_categories': len(all_cats),\n                'summary_length': summary_length,\n                'avg_sentence_length': round(avg_sentence_length, 2),\n                'last_revision_date': last_rev,\n                'num_languages': len(langs),\n                'languages': langs,\n                'rating': rating\n            }\n            \n            # Validate article data\n            if validate_article_data(article):\n                results.append(article)\n            else:\n                # Mark for retry\n                results.append(None)\n        \n        return results\n        \n    except Exception as e:\n        if retry_count < 3:\n            print(f\"  Retry {retry_count + 1}/3 for batch (Exception: {e})\")\n            time.sleep(2 ** retry_count)\n            return fetch_article_data_batch(page_ids, retry_count + 1)\n        else:\n            print(f\"  ERROR: {e} - batch failed after 3 retries\")\n            return []\n\n\ndef batch_fetch_metadata(page_ids, batch_size=50):\n    \"\"\"Fetch metadata for all articles using batched synchronous requests\"\"\"\n    from tqdm.auto import tqdm\n    \n    all_results = []\n    failed_page_ids = []\n    \n    # Split into batches\n    batches = [page_ids[i:i+batch_size] for i in range(0, len(page_ids), batch_size)]\n    \n    for batch in tqdm(batches, desc=\"Fetching batches\"):\n        batch_results = fetch_article_data_batch(batch)\n        \n        # Separate valid and invalid results\n        for i, result in enumerate(batch_results):\n            if result is None:\n                # This article failed validation - retry individually\n                failed_page_ids.append(batch[i])\n            else:\n                all_results.append(result)\n        \n        # Rate limiting between batches\n        time.sleep(0.5)\n    \n    # Retry failed articles individually\n    if failed_page_ids:\n        print(f\"\\n\\nRetrying {len(failed_page_ids)} failed articles individually...\")\n        for page_id in tqdm(failed_page_ids, desc=\"Retrying failed articles\"):\n            retry_results = fetch_article_data_batch([page_id])\n            if retry_results and retry_results[0] is not None:\n                all_results.append(retry_results[0])\n            else:\n                print(f\"  PERMANENTLY FAILED: page_id {page_id}\")\n            time.sleep(0.5)\n    \n    return all_results\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nimport pandas as pd\n\n# Load page IDs from CSV\ndf = pd.read_csv('article_names.csv')\npage_ids = df['page_id'].dropna().astype(int).tolist()\n\nprint(f\"Fetching metadata for {len(page_ids)} articles...\")\n\n# Run synchronous code\nstart_time = time.perf_counter()\narticle_data = batch_fetch_metadata(page_ids)\nend_time = time.perf_counter()\n\nprint(f\"\\nCompleted in {end_time - start_time:.1f} seconds\")\nprint(f\"Successfully fetched: {len(article_data)} articles\")\nprint(f\"Failed/missing: {len(page_ids) - len(article_data)} articles\")\n","metadata":{},"execution_count":null,"outputs":[{"data":{"text/plain":"Fetching metadata for 22633 articles...\n"},"execution_count":null,"metadata":{},"output_type":"execute_result"},{"data":{"text/plain":"Fetching batches:   0%|          | 0/453 [00:00<?, ?it/s]"},"execution_count":null,"metadata":{},"output_type":"execute_result"},{"data":{"text/plain":"\n\nRetrying 4071 failed articles individually...\n"},"execution_count":null,"metadata":{},"output_type":"execute_result"},{"data":{"text/plain":"Retrying failed articles:   0%|          | 0/4071 [00:00<?, ?it/s]"},"execution_count":null,"metadata":{},"output_type":"execute_result"},{"data":{"text/plain":"  PERMANENTLY FAILED: page_id 57408800\n  PERMANENTLY FAILED: page_id 3422745\n  PERMANENTLY FAILED: page_id 16113811\n  PERMANENTLY FAILED: page_id 5906740\n  PERMANENTLY FAILED: page_id 28660110\n  PERMANENTLY FAILED: page_id 3029813\n  PERMANENTLY FAILED: page_id 9539776\n  PERMANENTLY FAILED: page_id 2998200\n  PERMANENTLY FAILED: page_id 2998223\n  PERMANENTLY FAILED: page_id 55333032\n  PERMANENTLY FAILED: page_id 9720285\n  PERMANENTLY FAILED: page_id 79433201\n  PERMANENTLY FAILED: page_id 38549976\n  PERMANENTLY FAILED: page_id 357936\n  PERMANENTLY FAILED: page_id 1594704\n  PERMANENTLY FAILED: page_id 27206536\n  PERMANENTLY FAILED: page_id 56763423\n  PERMANENTLY FAILED: page_id 19376773\n  PERMANENTLY FAILED: page_id 14060128\n  PERMANENTLY FAILED: page_id 36089133\n  PERMANENTLY FAILED: page_id 2024813\n  PERMANENTLY FAILED: page_id 938690\n  PERMANENTLY FAILED: page_id 52727451\n  PERMANENTLY FAILED: page_id 8712327\n  PERMANENTLY FAILED: page_id 2618273\n  PERMANENTLY FAILED: page_id 28862318\n  PERMANENTLY FAILED: page_id 10626167\n  PERMANENTLY FAILED: page_id 395618\n  PERMANENTLY FAILED: page_id 848647\n  PERMANENTLY FAILED: page_id 7243007\n  PERMANENTLY FAILED: page_id 17650336\n  PERMANENTLY FAILED: page_id 10849593\n  PERMANENTLY FAILED: page_id 23854907\n  PERMANENTLY FAILED: page_id 234742\n  PERMANENTLY FAILED: page_id 26955043\n  PERMANENTLY FAILED: page_id 33450382\n  PERMANENTLY FAILED: page_id 20768218\n  PERMANENTLY FAILED: page_id 3224505\n  PERMANENTLY FAILED: page_id 12109052\n  PERMANENTLY FAILED: page_id 385108\n  PERMANENTLY FAILED: page_id 23903452\n  PERMANENTLY FAILED: page_id 156905\n  PERMANENTLY FAILED: page_id 37891052\n  PERMANENTLY FAILED: page_id 62751657\n  PERMANENTLY FAILED: page_id 15958655\n  PERMANENTLY FAILED: page_id 1089323\n  PERMANENTLY FAILED: page_id 16189891\n  PERMANENTLY FAILED: page_id 69378652\n  PERMANENTLY FAILED: page_id 2680992\n  PERMANENTLY FAILED: page_id 5056573\n  PERMANENTLY FAILED: page_id 5056577\n  PERMANENTLY FAILED: page_id 5056580\n  PERMANENTLY FAILED: page_id 316420\n  PERMANENTLY FAILED: page_id 15496078\n  PERMANENTLY FAILED: page_id 12732244\n  PERMANENTLY FAILED: page_id 18535298\n  PERMANENTLY FAILED: page_id 23627376\n  PERMANENTLY FAILED: page_id 2543940\n  PERMANENTLY FAILED: page_id 29628845\n  PERMANENTLY FAILED: page_id 1138939\n  PERMANENTLY FAILED: page_id 407183\n  PERMANENTLY FAILED: page_id 960368\n  PERMANENTLY FAILED: page_id 14013088\n  PERMANENTLY FAILED: page_id 1205227\n  PERMANENTLY FAILED: page_id 4171996\n  PERMANENTLY FAILED: page_id 963416\n  PERMANENTLY FAILED: page_id 1195986\n  PERMANENTLY FAILED: page_id 3050980\n  PERMANENTLY FAILED: page_id 2054549\n  PERMANENTLY FAILED: page_id 25303621\n  PERMANENTLY FAILED: page_id 652545\n  PERMANENTLY FAILED: page_id 20868780\n  PERMANENTLY FAILED: page_id 1521905\n  PERMANENTLY FAILED: page_id 4006097\n  PERMANENTLY FAILED: page_id 1105127\n  PERMANENTLY FAILED: page_id 8865925\n  PERMANENTLY FAILED: page_id 804775\n  PERMANENTLY FAILED: page_id 22938748\n  PERMANENTLY FAILED: page_id 981830\n  PERMANENTLY FAILED: page_id 1550513\n  PERMANENTLY FAILED: page_id 32766150\n  PERMANENTLY FAILED: page_id 17330648\n  PERMANENTLY FAILED: page_id 62751703\n  PERMANENTLY FAILED: page_id 33604430\n  PERMANENTLY FAILED: page_id 6764663\n  PERMANENTLY FAILED: page_id 414820\n  PERMANENTLY FAILED: page_id 11606849\n  PERMANENTLY FAILED: page_id 2550639\n  PERMANENTLY FAILED: page_id 22762937\n  PERMANENTLY FAILED: page_id 26847738\n  PERMANENTLY FAILED: page_id 7142551\n  PERMANENTLY FAILED: page_id 61434958\n  PERMANENTLY FAILED: page_id 14932804\n  PERMANENTLY FAILED: page_id 30939449\n  PERMANENTLY FAILED: page_id 9088\n  PERMANENTLY FAILED: page_id 46700898\n  PERMANENTLY FAILED: page_id 18144124\n  PERMANENTLY FAILED: page_id 4231375\n  PERMANENTLY FAILED: page_id 54935\n  PERMANENTLY FAILED: page_id 2210115\n  PERMANENTLY FAILED: page_id 19692883\n  PERMANENTLY FAILED: page_id 70515234\n  PERMANENTLY FAILED: page_id 39331965\n  PERMANENTLY FAILED: page_id 25858218\n  PERMANENTLY FAILED: page_id 2052439\n  PERMANENTLY FAILED: page_id 155435\n  PERMANENTLY FAILED: page_id 7621236\n  PERMANENTLY FAILED: page_id 794947\n  PERMANENTLY FAILED: page_id 3857911\n  PERMANENTLY FAILED: page_id 2889688\n  PERMANENTLY FAILED: page_id 71706658\n  PERMANENTLY FAILED: page_id 54421727\n  PERMANENTLY FAILED: page_id 69854233\n  PERMANENTLY FAILED: page_id 75371403\n  PERMANENTLY FAILED: page_id 580326\n  PERMANENTLY FAILED: page_id 4016238\n  PERMANENTLY FAILED: page_id 5963076\n  PERMANENTLY FAILED: page_id 2138984\n  PERMANENTLY FAILED: page_id 3124699\n  PERMANENTLY FAILED: page_id 3174931\n  PERMANENTLY FAILED: page_id 75061738\n  PERMANENTLY FAILED: page_id 808304\n  PERMANENTLY FAILED: page_id 80115485\n  PERMANENTLY FAILED: page_id 256437\n  PERMANENTLY FAILED: page_id 22403456\n  PERMANENTLY FAILED: page_id 626456\n  PERMANENTLY FAILED: page_id 34149847\n  PERMANENTLY FAILED: page_id 12866450\n  PERMANENTLY FAILED: page_id 1035305\n  PERMANENTLY FAILED: page_id 185630\n  PERMANENTLY FAILED: page_id 1874268\n  PERMANENTLY FAILED: page_id 36385488\n  PERMANENTLY FAILED: page_id 106147\n  PERMANENTLY FAILED: page_id 14347450\n  PERMANENTLY FAILED: page_id 2878214\n  PERMANENTLY FAILED: page_id 5294440\n  PERMANENTLY FAILED: page_id 174275\n  PERMANENTLY FAILED: page_id 1831919\n  PERMANENTLY FAILED: page_id 4072577\n  PERMANENTLY FAILED: page_id 65168679\n  PERMANENTLY FAILED: page_id 2145637\n  PERMANENTLY FAILED: page_id 27603441\n  PERMANENTLY FAILED: page_id 75610274\n  PERMANENTLY FAILED: page_id 52596745\n  PERMANENTLY FAILED: page_id 26501048\n  PERMANENTLY FAILED: page_id 38429225\n  PERMANENTLY FAILED: page_id 11856431\n  PERMANENTLY FAILED: page_id 477542\n  PERMANENTLY FAILED: page_id 12903627\n  PERMANENTLY FAILED: page_id 383666\n  PERMANENTLY FAILED: page_id 62551321\n  PERMANENTLY FAILED: page_id 530770\n  PERMANENTLY FAILED: page_id 12207859\n  PERMANENTLY FAILED: page_id 46298699\n  PERMANENTLY FAILED: page_id 53100938\n  PERMANENTLY FAILED: page_id 30506455\n  PERMANENTLY FAILED: page_id 46506\n  PERMANENTLY FAILED: page_id 182861\n  PERMANENTLY FAILED: page_id 2996450\n  PERMANENTLY FAILED: page_id 39269237\n  PERMANENTLY FAILED: page_id 1834022\n  PERMANENTLY FAILED: page_id 12952248\n  PERMANENTLY FAILED: page_id 3332547\n  PERMANENTLY FAILED: page_id 45073074\n  PERMANENTLY FAILED: page_id 67994287\n  PERMANENTLY FAILED: page_id 371767\n  PERMANENTLY FAILED: page_id 19652654\n  PERMANENTLY FAILED: page_id 19652679\n  PERMANENTLY FAILED: page_id 53780136\n  PERMANENTLY FAILED: page_id 21449830\n  PERMANENTLY FAILED: page_id 34044638\n  PERMANENTLY FAILED: page_id 29247833\n  PERMANENTLY FAILED: page_id 591419\n  PERMANENTLY FAILED: page_id 26449728\n  PERMANENTLY FAILED: page_id 21890947\n  PERMANENTLY FAILED: page_id 216212\n  PERMANENTLY FAILED: page_id 32091375\n  PERMANENTLY FAILED: page_id 26027347\n  PERMANENTLY FAILED: page_id 70061511\n  PERMANENTLY FAILED: page_id 76396083\n  PERMANENTLY FAILED: page_id 14918858\n  PERMANENTLY FAILED: page_id 17706329\n  PERMANENTLY FAILED: page_id 30688995\n  PERMANENTLY FAILED: page_id 8878047\n  PERMANENTLY FAILED: page_id 4400396\n  PERMANENTLY FAILED: page_id 14066076\n  PERMANENTLY FAILED: page_id 1948282\n  PERMANENTLY FAILED: page_id 450704\n  PERMANENTLY FAILED: page_id 10570337\n  PERMANENTLY FAILED: page_id 54580241\n  PERMANENTLY FAILED: page_id 11515306\n  PERMANENTLY FAILED: page_id 21585003\n  PERMANENTLY FAILED: page_id 1195269\n  PERMANENTLY FAILED: page_id 905376\n  PERMANENTLY FAILED: page_id 1426710\n  PERMANENTLY FAILED: page_id 42547594\n  PERMANENTLY FAILED: page_id 2701391\n  PERMANENTLY FAILED: page_id 10476944\n  PERMANENTLY FAILED: page_id 6097400\n  PERMANENTLY FAILED: page_id 22114201\n  PERMANENTLY FAILED: page_id 22763892\n  PERMANENTLY FAILED: page_id 1345943\n  PERMANENTLY FAILED: page_id 17708\n  PERMANENTLY FAILED: page_id 21492906\n  PERMANENTLY FAILED: page_id 1064601\n  PERMANENTLY FAILED: page_id 7257637\n  PERMANENTLY FAILED: page_id 75252708\n  PERMANENTLY FAILED: page_id 17355940\n  PERMANENTLY FAILED: page_id 33936897\n  PERMANENTLY FAILED: page_id 34269372\n  PERMANENTLY FAILED: page_id 264777\n  PERMANENTLY FAILED: page_id 69158144\n  PERMANENTLY FAILED: page_id 34661458\n  PERMANENTLY FAILED: page_id 1974881\n  PERMANENTLY FAILED: page_id 984144\n  PERMANENTLY FAILED: page_id 1936961\n  PERMANENTLY FAILED: page_id 3892146\n  PERMANENTLY FAILED: page_id 27466917\n  PERMANENTLY FAILED: page_id 2805554\n  PERMANENTLY FAILED: page_id 76962314\n  PERMANENTLY FAILED: page_id 6764680\n  PERMANENTLY FAILED: page_id 5116560\n  PERMANENTLY FAILED: page_id 8112048\n  PERMANENTLY FAILED: page_id 49330300\n  PERMANENTLY FAILED: page_id 63961237\n  PERMANENTLY FAILED: page_id 32410701\n  PERMANENTLY FAILED: page_id 55355297\n  PERMANENTLY FAILED: page_id 304363\n  PERMANENTLY FAILED: page_id 34424100\n  PERMANENTLY FAILED: page_id 3874499\n  PERMANENTLY FAILED: page_id 32097887\n  PERMANENTLY FAILED: page_id 76962362\n  PERMANENTLY FAILED: page_id 11449077\n  PERMANENTLY FAILED: page_id 294001\n  PERMANENTLY FAILED: page_id 619600\n  PERMANENTLY FAILED: page_id 12781303\n  PERMANENTLY FAILED: page_id 4543979\n  PERMANENTLY FAILED: page_id 1050154\n  PERMANENTLY FAILED: page_id 40512427\n  PERMANENTLY FAILED: page_id 10924739\n  PERMANENTLY FAILED: page_id 17336497\n  PERMANENTLY FAILED: page_id 1259315\n  PERMANENTLY FAILED: page_id 983142\n  PERMANENTLY FAILED: page_id 23436050\n  PERMANENTLY FAILED: page_id 7146265\n  PERMANENTLY FAILED: page_id 7905545\n  PERMANENTLY FAILED: page_id 202554\n  PERMANENTLY FAILED: page_id 7546319\n  PERMANENTLY FAILED: page_id 11978635\n  PERMANENTLY FAILED: page_id 37711315\n  PERMANENTLY FAILED: page_id 475624\n  PERMANENTLY FAILED: page_id 2999148\n  PERMANENTLY FAILED: page_id 638902\n  PERMANENTLY FAILED: page_id 5427496\n  PERMANENTLY FAILED: page_id 24817874\n  PERMANENTLY FAILED: page_id 492624\n  PERMANENTLY FAILED: page_id 31289071\n  PERMANENTLY FAILED: page_id 2398445\n  PERMANENTLY FAILED: page_id 9334584\n  PERMANENTLY FAILED: page_id 17489950\n  PERMANENTLY FAILED: page_id 16081248\n  PERMANENTLY FAILED: page_id 13694183\n  PERMANENTLY FAILED: page_id 23270864\n  PERMANENTLY FAILED: page_id 50186919\n  PERMANENTLY FAILED: page_id 3699717\n  PERMANENTLY FAILED: page_id 3303852\n  PERMANENTLY FAILED: page_id 6183832\n  PERMANENTLY FAILED: page_id 294342\n  PERMANENTLY FAILED: page_id 2709854\n  PERMANENTLY FAILED: page_id 22564463\n  PERMANENTLY FAILED: page_id 355616\n  PERMANENTLY FAILED: page_id 62826820\n  PERMANENTLY FAILED: page_id 931931\n  PERMANENTLY FAILED: page_id 20227296\n  PERMANENTLY FAILED: page_id 45522296\n  PERMANENTLY FAILED: page_id 12216605\n  PERMANENTLY FAILED: page_id 2118313\n  PERMANENTLY FAILED: page_id 1147315\n  PERMANENTLY FAILED: page_id 42285543\n  PERMANENTLY FAILED: page_id 1213945\n  PERMANENTLY FAILED: page_id 1855694\n  PERMANENTLY FAILED: page_id 1325970\n  PERMANENTLY FAILED: page_id 27642358\n  PERMANENTLY FAILED: page_id 28408326\n  PERMANENTLY FAILED: page_id 25707216\n  PERMANENTLY FAILED: page_id 28887815\n  PERMANENTLY FAILED: page_id 52471230\n  PERMANENTLY FAILED: page_id 575122\n  PERMANENTLY FAILED: page_id 14056750\n  PERMANENTLY FAILED: page_id 78104233\n  PERMANENTLY FAILED: page_id 51119176\n  PERMANENTLY FAILED: page_id 9404381\n  PERMANENTLY FAILED: page_id 3430456\n  PERMANENTLY FAILED: page_id 12890462\n  PERMANENTLY FAILED: page_id 1943826\n  PERMANENTLY FAILED: page_id 14759195\n  PERMANENTLY FAILED: page_id 8820476\n  PERMANENTLY FAILED: page_id 4983068\n  PERMANENTLY FAILED: page_id 22404561\n  PERMANENTLY FAILED: page_id 38144410\n  PERMANENTLY FAILED: page_id 21270640\n  PERMANENTLY FAILED: page_id 2846171\n  PERMANENTLY FAILED: page_id 3839535\n  PERMANENTLY FAILED: page_id 3917849\n  PERMANENTLY FAILED: page_id 19753755\n  PERMANENTLY FAILED: page_id 1647566\n  PERMANENTLY FAILED: page_id 38427543\n  PERMANENTLY FAILED: page_id 232250\n  PERMANENTLY FAILED: page_id 70797454\n  PERMANENTLY FAILED: page_id 14909059\n  PERMANENTLY FAILED: page_id 326649\n  PERMANENTLY FAILED: page_id 5029350\n  PERMANENTLY FAILED: page_id 9169552\n  PERMANENTLY FAILED: page_id 14810633\n  PERMANENTLY FAILED: page_id 12435574\n  PERMANENTLY FAILED: page_id 4939107\n  PERMANENTLY FAILED: page_id 206791\n  PERMANENTLY FAILED: page_id 636165\n  PERMANENTLY FAILED: page_id 4787234\n  PERMANENTLY FAILED: page_id 2722305\n  PERMANENTLY FAILED: page_id 3094289\n  PERMANENTLY FAILED: page_id 9285554\n  PERMANENTLY FAILED: page_id 2476391\n  PERMANENTLY FAILED: page_id 31874567\n  PERMANENTLY FAILED: page_id 193029\n  PERMANENTLY FAILED: page_id 28832640\n  PERMANENTLY FAILED: page_id 42165490\n  PERMANENTLY FAILED: page_id 76908565\n  PERMANENTLY FAILED: page_id 55045372\n  PERMANENTLY FAILED: page_id 6764641\n  PERMANENTLY FAILED: page_id 536077\n  PERMANENTLY FAILED: page_id 557708\n  PERMANENTLY FAILED: page_id 2968815\n  PERMANENTLY FAILED: page_id 31392788\n  PERMANENTLY FAILED: page_id 2306818\n  PERMANENTLY FAILED: page_id 18339775\n  PERMANENTLY FAILED: page_id 63679808\n  PERMANENTLY FAILED: page_id 2692187\n  PERMANENTLY FAILED: page_id 555306\n  PERMANENTLY FAILED: page_id 3956282\n  PERMANENTLY FAILED: page_id 53054384\n  PERMANENTLY FAILED: page_id 1431657\n  PERMANENTLY FAILED: page_id 996889\n  PERMANENTLY FAILED: page_id 4466518\n  PERMANENTLY FAILED: page_id 337037\n  PERMANENTLY FAILED: page_id 56645750\n  PERMANENTLY FAILED: page_id 45091778\n  PERMANENTLY FAILED: page_id 50751158\n  PERMANENTLY FAILED: page_id 1042577\n  PERMANENTLY FAILED: page_id 26973077\n  PERMANENTLY FAILED: page_id 54318780\n  PERMANENTLY FAILED: page_id 11607543\n  PERMANENTLY FAILED: page_id 42412033\n  PERMANENTLY FAILED: page_id 22289785\n  PERMANENTLY FAILED: page_id 25571058\n  PERMANENTLY FAILED: page_id 3922844\n  PERMANENTLY FAILED: page_id 582177\n  PERMANENTLY FAILED: page_id 50008455\n  PERMANENTLY FAILED: page_id 331345\n  PERMANENTLY FAILED: page_id 3057380\n  PERMANENTLY FAILED: page_id 179116\n  PERMANENTLY FAILED: page_id 75060871\n  PERMANENTLY FAILED: page_id 7207145\n  PERMANENTLY FAILED: page_id 73454078\n  PERMANENTLY FAILED: page_id 8934065\n  PERMANENTLY FAILED: page_id 18004853\n  PERMANENTLY FAILED: page_id 21001188\n  PERMANENTLY FAILED: page_id 1146515\n  PERMANENTLY FAILED: page_id 475825\n  PERMANENTLY FAILED: page_id 2903869\n  PERMANENTLY FAILED: page_id 33483149\n  PERMANENTLY FAILED: page_id 939034\n  PERMANENTLY FAILED: page_id 20372080\n  PERMANENTLY FAILED: page_id 1482430\n  PERMANENTLY FAILED: page_id 17188661\n  PERMANENTLY FAILED: page_id 17673161\n  PERMANENTLY FAILED: page_id 27361900\n  PERMANENTLY FAILED: page_id 2172315\n  PERMANENTLY FAILED: page_id 76287077\n  PERMANENTLY FAILED: page_id 76762395\n  PERMANENTLY FAILED: page_id 4223943\n\nCompleted in 6674.0 seconds\nSuccessfully fetched: 22252 articles\nFailed/missing: 381 articles\n"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","source":"article_data = pd.DataFrame(article_data)\narticle_data.to_csv('article_data.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"people_keywords = {'births', 'deaths', 'people', 'living'}\nletter_keywords = {'letters', 'letter', 'alphabet'}\nevent_keywords = {'events', 'battles', 'wars', 'conflicts', 'disasters', 'treaties', 'history'}\norg_keywords = {'organizations', 'organisations', 'companies', 'institutions', 'universities', 'agencies'}\nother_keywords = { 'book'}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the saved article data to examine structure\nimport pandas as pd\ndf_check = pd.read_csv('article_data.csv')\nprint(f\"Shape: {df_check.shape}\")\nprint(f\"\\nColumns: {df_check.columns.tolist()}\")\nprint(f\"\\nSample of categories column:\")\nprint(df_check['all_categories'].head(2))\n","metadata":{},"execution_count":null,"outputs":[{"data":{"text/plain":"Shape: (22252, 17)\n\nColumns: ['page_id', 'title', 'url', 'content_length', 'num_links', 'num_sections', 'num_references', 'all_categories', 'meta_categories', 'science_categories', 'num_categories', 'summary_length', 'avg_sentence_length', 'last_revision_date', 'num_languages', 'languages', 'rating']\n\nSample of categories column:\n0    ['1802 in science', 'Articles containing Ancie...\n1    ['Actuarial science', 'Articles with short des...\nName: all_categories, dtype: object\n"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","source":"import ast\n\ndef should_filter_article(categories_str, filter_keywords):\n    \"\"\"\n    Check if any category contains any of the filter keywords.\n    Returns True if article should be REMOVED.\n    \"\"\"\n    if pd.isna(categories_str) or categories_str == '[]':\n        return False\n    \n    # Parse the string representation of list into actual list\n    try:\n        categories = ast.literal_eval(categories_str)\n    except:\n        return False\n    \n    # Check each category against all filter keywords\n    for category in categories:\n        category_lower = category.lower()\n        for keyword in filter_keywords:\n            if keyword in category_lower:\n                return True\n    \n    return False\n\n# Combine all filter keywords\nall_filter_keywords = (\n    people_keywords | \n    letter_keywords | \n    event_keywords | \n    org_keywords |\n    other_keywords\n)\n\nprint(f\"Total filter keywords: {len(all_filter_keywords)}\")\nprint(f\"Keywords: {sorted(all_filter_keywords)}\")\n","metadata":{},"execution_count":null,"outputs":[{"data":{"text/plain":"Total filter keywords: 21\nKeywords: ['agencies', 'alphabet', 'battles', 'births', 'book', 'companies', 'conflicts', 'deaths', 'disasters', 'events', 'history', 'institutions', 'letter', 'letters', 'living', 'organisations', 'organizations', 'people', 'treaties', 'universities', 'wars']\n"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","source":"# Load the data\ndf = pd.read_csv('article_data.csv')\n\nprint(f\"Original dataset: {len(df)} articles\")\n\n# Apply filter - mark articles to remove\ndf['should_filter'] = df['all_categories'].apply(\n    lambda x: should_filter_article(x, all_filter_keywords)\n)\n\n# Show filtering stats\nfiltered_out = df['should_filter'].sum()\nprint(f\"Articles to filter out: {filtered_out} ({filtered_out/len(df)*100:.1f}%)\")\nprint(f\"Articles remaining: {len(df) - filtered_out}\")\n\n# Create clean dataset\ndf_clean = df[~df['should_filter']].drop(columns=['should_filter'])\n\n# Save filtered dataset\ndf_clean.to_csv('article_data_filtered.csv', index=False)\nprint(f\"\\nSaved filtered dataset to 'article_data_filtered.csv'\")\n","metadata":{},"execution_count":null,"outputs":[{"data":{"text/plain":"Original dataset: 22252 articles\nArticles to filter out: 1973 (8.9%)\nArticles remaining: 20279\n\nSaved filtered dataset to 'article_data_filtered.csv'\n"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","source":"# Load the category-filtered dataset\ndf_filtered = pd.read_csv('article_data_filtered.csv')\n\nprint(f\"After category filtering: {len(df_filtered):,} articles\")\n\n# Check if rating column exists\nif 'rating' not in df_filtered.columns:\n    print(\"\\n⚠️  ERROR: 'rating' column not found in dataset\")\n    print(\"The metadata fetch may not have completed successfully.\")\nelse:\n    # Filter to keep only articles WITH ratings\n    df_with_ratings = df_filtered[df_filtered['rating'].notna()].copy()\n    \n    print(f\"\\nArticles WITH ratings: {len(df_with_ratings):,} ({len(df_with_ratings)/len(df_filtered)*100:.1f}%)\")\n    print(f\"Articles WITHOUT ratings: {(~df_filtered['rating'].notna()).sum():,}\")\n    \n    # Show rating distribution\n    print(f\"\\nRating distribution:\")\n    rating_counts = df_with_ratings['rating'].value_counts().sort_index()\n    for rating, count in rating_counts.items():\n        print(f\"  {rating}: {count:,}\")\n    \n    # Save final dataset\n    df_with_ratings.to_csv('wikipedia.csv', index=False)\n    print(f\"\\n✓ Saved final dataset: {len(df_with_ratings):,} articles → 'wikipedia.csv'\")\n","metadata":{},"execution_count":null,"outputs":[{"data":{"text/plain":"After category filtering: 20,279 articles\n\nArticles WITH ratings: 16,472 (81.2%)\nArticles WITHOUT ratings: 3,807\n\nRating distribution:\n  B: 2,596\n  C: 5,855\n  FA: 136\n  GA: 402\n  START: 5,929\n  STUB: 1,554\n\n✓ Saved final dataset: 16,472 articles → 'wikipedia.csv'\n"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","source":"import pandas as pd\n\n# Load the old ML-ready dataset\ndf_old = pd.read_csv('wikipedia_dataset_ml_ready.csv')\n\n# Load our current final dataset\ndf_new = pd.read_csv('article_data_final.csv')\n\nprint(\"OLD DATASET (wikipedia_dataset_ml_ready.csv):\")\nprint(\"=\" * 70)\nprint(f\"Articles: {len(df_old):,}\")\nprint(f\"Columns: {df_old.columns.tolist()}\")\nprint(f\"\\nShape: {df_old.shape}\")\n\nprint(\"\\n\\nNEW DATASET (article_data_final.csv):\")\nprint(\"=\" * 70)\nprint(f\"Articles: {len(df_new):,}\")\nprint(f\"Columns: {df_new.columns.tolist()}\")\nprint(f\"\\nShape: {df_new.shape}\")\n\nprint(\"\\n\\nDIFFERENCE:\")\nprint(\"=\" * 70)\nprint(f\"Article count difference: {len(df_old) - len(df_new):,}\")\n\n# Check rating distribution in old dataset\nif 'rating' in df_old.columns:\n    print(f\"\\nOld dataset rating distribution:\")\n    print(df_old['rating'].value_counts().sort_index())\n    \nif 'rating' in df_new.columns:\n    print(f\"\\nNew dataset rating distribution:\")\n    print(df_new['rating'].value_counts().sort_index())\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ast\n\n# Load the filtered dataset\ndf_final = pd.read_csv('article_data_final.csv')\n\n# Check for articles with 'agencies' in their categories\narticles_with_agencies = []\n\nfor idx, row in df_final.iterrows():\n    try:\n        categories = ast.literal_eval(row['all_categories'])\n        for cat in categories:\n            if 'agencies' in cat.lower():\n                articles_with_agencies.append({\n                    'title': row['title'],\n                    'category': cat\n                })\n                break\n    except:\n        continue\n\nprint(f\"Articles with 'agencies' in categories: {len(articles_with_agencies)}\")\nprint(\"\\nExamples:\")\nfor item in articles_with_agencies[:10]:\n    print(f\"  - {item['title']}\")\n    print(f\"    Category: {item['category']}\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ast\n\n# Load the final dataset\ndf_final = pd.read_csv('article_data_final.csv')\n\n# Find ANSMET article\nansmet = df_final[df_final['page_id'] == 1851775]\n\nif len(ansmet) > 0:\n    print(f\"FOUND: {ansmet.iloc[0]['title']}\")\n    print(f\"Page ID: {ansmet.iloc[0]['page_id']}\")\n    print(f\"\\nAll categories:\")\n    print(\"=\" * 70)\n    \n    try:\n        categories = ast.literal_eval(ansmet.iloc[0]['all_categories'])\n        for i, cat in enumerate(categories, 1):\n            print(f\"{i}. {cat}\")\n            # Check if it contains 'agencies'\n            if 'agencies' in cat.lower():\n                print(f\"   ^^^ CONTAINS 'agencies'!\")\n    except Exception as e:\n        print(f\"Error parsing categories: {e}\")\nelse:\n    print(\"Article with page_id 1851775 NOT FOUND in article_data_final.csv\")\n    \n    # Check in the pre-rating-filter dataset\n    df_filtered = pd.read_csv('article_data_filtered.csv')\n    ansmet_filtered = df_filtered[df_filtered['page_id'] == 1851775]\n    \n    if len(ansmet_filtered) > 0:\n        print(\"\\nBut it WAS found in article_data_filtered.csv (before rating filter)\")\n        print(\"This means it was filtered out due to missing rating, not categories\")\n    else:\n        print(\"\\nAlso NOT found in article_data_filtered.csv\")\n        print(\"Checking original article_data.csv...\")\n        \n        df_original = pd.read_csv('article_data.csv')\n        ansmet_original = df_original[df_original['page_id'] == 1851775]\n        \n        if len(ansmet_original) > 0:\n            print(\"\\nFOUND in article_data.csv (before category filtering)!\")\n            print(f\"Title: {ansmet_original.iloc[0]['title']}\")\n            print(f\"\\nCategories:\")\n            try:\n                categories = ast.literal_eval(ansmet_original.iloc[0]['all_categories'])\n                for i, cat in enumerate(categories, 1):\n                    print(f\"{i}. {cat}\")\n                    if 'agencies' in cat.lower():\n                        print(f\"   ^^^ CONTAINS 'agencies' - SHOULD HAVE BEEN FILTERED!\")\n            except:\n                pass\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Look for should_remove_article function definition\nfor i, cell in enumerate(notebook['cells']):\n    if cell['cell_type'] == 'code':\n        source = ''.join(cell['source'])\n        if 'should_remove_article' in source and 'def ' in source:\n            print(f\"Cell {i} - should_remove_article definition:\")\n            print(\"=\" * 70)\n            print(source)\n            print(\"\\n\" + \"=\" * 70)\n            break\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Recreate old word-based matching\ndef should_remove_OLD(categories_str, keywords):\n    \"\"\"Old approach: word-based matching\"\"\"\n    if pd.isna(categories_str) or categories_str == '[]':\n        return False\n    \n    try:\n        categories = ast.literal_eval(categories_str)\n    except:\n        return False\n    \n    for cat in categories:\n        # Split category into words\n        words = {word.lower() for word in cat.split()}\n        # Check for intersection with keywords\n        if words & keywords:\n            return True\n    return False\n\n# Test both approaches\ndf_test = pd.read_csv('article_data.csv')\n\n# Apply OLD word-based filter\nold_filter = df_test['all_categories'].apply(\n    lambda x: should_remove_OLD(x, all_filter_keywords)\n)\n\n# Apply NEW substring filter (already done in C19, but let's recalculate)\nnew_filter = df_test['all_categories'].apply(\n    lambda x: should_filter_article(x, all_filter_keywords)\n)\n\nprint(\"Comparison:\")\nprint(f\"Old word-based:    {old_filter.sum()} filtered out, {len(df_test) - old_filter.sum()} kept\")\nprint(f\"New substring:     {new_filter.sum()} filtered out, {len(df_test) - new_filter.sum()} kept\")\nprint(f\"\\nDifference: {abs(old_filter.sum() - new_filter.sum())} articles\")\n","metadata":{},"execution_count":null,"outputs":[]}],"metadata":{"orig_nbformat":4,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"hex_info":{"author":"Lucas Campagnaro","project_id":"019bae67-af4d-7000-baed-c8d253b14659","version":"draft","exported_date":"Fri Jan 23 2026 19:52:39 GMT+0000 (Coordinated Universal Time)"}},"nbformat":4,"nbformat_minor":4}